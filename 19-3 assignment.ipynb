{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "738a1fde-0068-45e1-9e01-dd8f35911edc",
   "metadata": {},
   "source": [
    "## Q1. What is Min-Max scaling, and how is it used in data preprocessing? Provide an example to illustrate its application."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2117f919-7cd0-4560-97d1-60558055fed5",
   "metadata": {},
   "source": [
    "Min-max scaling, also known as feature scaling or normalization, is a data preprocessing technique that is used to transform the values of features in a dataset so that they have a common scale. This is done by subtracting the minimum value of each feature from all of its values and then dividing by the difference between the maximum and minimum values.\n",
    "\n",
    "For example, if a feature has a minimum value of 0 and a maximum value of 10, then min-max scaling will transform all of its values to the range 0 to 1. This makes it easier for machine learning algorithms to learn from the data, as they will not be sensitive to the different scales of the features.\n",
    "\n",
    "Min-max scaling is a common data preprocessing technique that is used in a variety of machine learning tasks, including classification, regression, and clustering. It is a relatively simple technique to implement, but it can be very effective in improving the performance of machine learning algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1d9f2c5-2ac5-4fb8-8888-4045ad80e952",
   "metadata": {},
   "source": [
    "Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "44a2e57e-c8f9-4437-a9da-46f545cb127f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>total_bill</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.291579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.152283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.375786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.431713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.450775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>239</th>\n",
       "      <td>0.543779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>240</th>\n",
       "      <td>0.505027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>241</th>\n",
       "      <td>0.410557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>242</th>\n",
       "      <td>0.308965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>243</th>\n",
       "      <td>0.329074</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>244 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     total_bill\n",
       "0      0.291579\n",
       "1      0.152283\n",
       "2      0.375786\n",
       "3      0.431713\n",
       "4      0.450775\n",
       "..          ...\n",
       "239    0.543779\n",
       "240    0.505027\n",
       "241    0.410557\n",
       "242    0.308965\n",
       "243    0.329074\n",
       "\n",
       "[244 rows x 1 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "df=sns.load_dataset('tips')\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "min_max=MinMaxScaler()\n",
    "df1=pd.DataFrame(min_max.fit_transform(df[['total_bill']]),columns=['total_bill'])\n",
    "df1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "901184bf-4f37-4226-a24f-2abdf38474e4",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Q2. What is the Unit Vector technique in feature scaling, and how does it differ from Min-Max scaling? Provide an example to illustrate its application."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49b6849a-0a58-4831-8b06-136f40cbb4d4",
   "metadata": {},
   "source": [
    "Unit Vector technique is a data preprocessing technique that is used to scale the features in a dataset so that they have a unit norm. This is done by dividing each feature by its norm. The norm of a feature is the length of the feature vector.\n",
    "\n",
    "For example, if a feature has a norm of 10, then unit vector scaling will divide all of its values by 10. This will make the feature vector have a length of 1.\n",
    "\n",
    "Unit vector scaling is a less common data preprocessing technique than min-max scaling. However, it can be useful in some cases, such as when the features are of different units or when the features have different scales.\n",
    "\n",
    "\n",
    "Here are some of the key differences between unit vector scaling and min-max scaling:\n",
    "\n",
    "1. Unit vector scaling transforms the features so that they have a unit norm, \n",
    "2. while min-max scaling transforms the features so that they have a common scale.\n",
    "3. Unit vector scaling does not change the distribution of the data, \n",
    "4. while min-max scaling can change the distribution of the data.\n",
    "5. Unit vector scaling is more sensitive to outliers than min-max scaling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99ddf700-04e6-4412-872e-217a7ed3f6d5",
   "metadata": {},
   "source": [
    "Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a6eb2803-0713-49fb-aa50-7fef72a1f2dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.213461</td>\n",
       "      <td>0.933894</td>\n",
       "      <td>0.286839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.156064</td>\n",
       "      <td>0.987747</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.171657</td>\n",
       "      <td>0.939731</td>\n",
       "      <td>0.295702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.267899</td>\n",
       "      <td>0.939386</td>\n",
       "      <td>0.213971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.231742</td>\n",
       "      <td>0.965592</td>\n",
       "      <td>0.118017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6428</th>\n",
       "      <td>0.160133</td>\n",
       "      <td>0.960800</td>\n",
       "      <td>0.226322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6429</th>\n",
       "      <td>0.307453</td>\n",
       "      <td>0.951563</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6430</th>\n",
       "      <td>0.250500</td>\n",
       "      <td>0.968117</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6431</th>\n",
       "      <td>0.183497</td>\n",
       "      <td>0.983020</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6432</th>\n",
       "      <td>0.242956</td>\n",
       "      <td>0.946580</td>\n",
       "      <td>0.212034</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6433 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             0         1         2\n",
       "0     0.213461  0.933894  0.286839\n",
       "1     0.156064  0.987747  0.000000\n",
       "2     0.171657  0.939731  0.295702\n",
       "3     0.267899  0.939386  0.213971\n",
       "4     0.231742  0.965592  0.118017\n",
       "...        ...       ...       ...\n",
       "6428  0.160133  0.960800  0.226322\n",
       "6429  0.307453  0.951563  0.000000\n",
       "6430  0.250500  0.968117  0.000000\n",
       "6431  0.183497  0.983020  0.000000\n",
       "6432  0.242956  0.946580  0.212034\n",
       "\n",
       "[6433 rows x 3 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import normalize\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "df=sns.load_dataset('taxis')\n",
    "pd.DataFrame(normalize(df[['distance','fare','tip']]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a99ba7f9-fdab-4002-a3ba-d16727a4f8fb",
   "metadata": {},
   "source": [
    "## Q3. What is PCA (Principle Component Analysis), and how is it used in dimensionality reduction? Provide an example to illustrate its application."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1550053-a5b3-492d-a28d-398835128ae7",
   "metadata": {},
   "source": [
    "Principal component analysis (PCA) is a statistical procedure that uses an orthogonal transformation to convert a set of observations of possibly correlated variables into a set of values of linearly uncorrelated variables called principal components. This transformation is defined in such a way that the first principal component has the largest possible variance (that is, accounts for as much of the variability in the data as possible), and each succeeding component in turn has the highest variance possible under the constraint that it is orthogonal to (i.e., uncorrelated with) the preceding components.\n",
    "\n",
    "PCA is a widely used dimensionality reduction technique. It can be used to reduce the number of features in a dataset without losing too much information. This can be helpful in machine learning tasks, as it can make the data easier to learn from and can improve the performance of machine learning algorithms.\n",
    "\n",
    "Here are some of the benefits of using PCA:\n",
    "\n",
    "1. It can reduce the number of features in a dataset: PCA can reduce the number of features in a dataset without losing too much information. This can make the data easier to learn from and can improve the performance of machine learning algorithms.\n",
    "2. It can reveal hidden patterns in the data: PCA can reveal hidden patterns in the data that may not be obvious when looking at the raw data. This can be helpful in understanding the data and in making better decisions.\n",
    "3. It is a relatively simple technique to implement: PCA is a relatively simple technique to implement. This makes it a good option for beginners who are new to dimensionality reduction.\n",
    "\n",
    "Here are some of the limitations of using PCA:\n",
    "\n",
    "1. It can lose information: PCA can lose information when the number of features is reduced. This means that the machine learning algorithms may not be able to learn as well from the data.\n",
    "2. It can be sensitive to outliers: PCA is sensitive to outliers. This means that if there are outliers in the data, they can have a significant impact on the results of PCA.\n",
    "3. It is not always the best choice for dimensionality reduction: PCA is not always the best choice for dimensionality reduction. There are other dimensionality reduction techniques that may be more appropriate for a particular dataset.\n",
    "\n",
    "Overall, PCA is a powerful dimensionality reduction technique that can be used to improve the performance of machine learning algorithms. However, it is important to be aware of the limitations of the technique and to choose the right dimensionality reduction technique for the specific datase\n",
    "\n",
    "Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f8f6e465-1c5e-47e9-b1cd-b05646c68c06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.85839102 0.98170948]\n",
      "[[-0.73639773  0.02823698]\n",
      " [-0.52898396 -0.64690963]\n",
      " [-0.42177524  0.7620437 ]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Create a dataset of 100 points in 3 dimensions.\n",
    "data = np.random.randn(100, 3)\n",
    "\n",
    "# Standardize the data.\n",
    "data = (data - np.mean(data, axis=0)) / np.std(data, axis=0)\n",
    "\n",
    "# Compute the covariance matrix.\n",
    "cov = np.cov(data.T)\n",
    "\n",
    "# Find the eigenvalues and eigenvectors of the covariance matrix.\n",
    "eigvals, eigvecs = np.linalg.eig(cov)\n",
    "\n",
    "# Retain the principal components with the largest eigenvalues.\n",
    "num_components = 2\n",
    "eigvals = eigvals[-num_components:]\n",
    "eigvecs = eigvecs[:, -num_components:]\n",
    "\n",
    "# Project the data onto the principal components.\n",
    "projected_data = np.dot(data, eigvecs)\n",
    "\n",
    "# Print the principal components.\n",
    "print(eigvals)\n",
    "print(eigvecs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c2fc977-9ea0-4d62-9d36-13128caa5d39",
   "metadata": {},
   "source": [
    "## Q4. What is the relationship between PCA and Feature Extraction, and how can PCA be used for Feature Extraction? Provide an example to illustrate this concept."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9307f31-a90a-40c6-b3de-9622d10eb29b",
   "metadata": {},
   "source": [
    "Principal component analysis (PCA) and feature extraction are both techniques used to reduce the dimensionality of data. However, they differ in their approach. PCA is a statistical technique that projects the data onto a lower-dimensional subspace that preserves as much of the variance in the data as possible. Feature extraction, on the other hand, is a more general technique that can be used to transform the data into a new set of features that are more informative or easier to work with.\n",
    "\n",
    "PCA can be used for feature extraction by projecting the data onto a lower-dimensional subspace that only contains the most important features. This can be helpful in machine learning tasks, as it can make the data easier to learn from and can improve the performance of machine learning algorithms.\n",
    "\n",
    "For example, let's say we have a dataset of images of faces. The images are represented as a matrix of pixels, so each image has a very large number of features (the number of pixels in the image). However, we know that most of the variation in the images is due to a few important features, such as the position of the eyes, nose, and mouth. PCA can be used to identify these important features and to reduce the number of features in the dataset to a more manageable number.\n",
    "\n",
    "The main steps of PCA for feature extraction are as follows:\n",
    "\n",
    "Standardize the data: The data is standardized so that the mean of each feature is 0 and the standard deviation is 1. This is done to ensure that the features are on the same scale and that they contribute equally to the analysis.\n",
    "Compute the covariance matrix: The covariance matrix is a square matrix that measures the covariance between each pair of features. The covariance between two features is a measure of how much they vary together.\n",
    "Find the eigenvalues and eigenvectors of the covariance matrix: The eigenvalues of the covariance matrix are the variances of the principal components. The eigenvectors of the covariance matrix are the directions of the principal components.\n",
    "Retain the principal components with the largest eigenvalues: The principal components are ordered by their eigenvalues. The principal components with the largest eigenvalues are the most important and should be retained.\n",
    "Project the data onto the principal components: The data is projected onto the principal components. This means that the data is represented as a new set of features that are a linear combination of the principal components.\n",
    "The new set of features that are created by PCA are called principal components. The principal components are ordered by their importance, with the first principal component being the most important and the last principal component being the least important. The principal components can be used as features in machine learning tasks.\n",
    "\n",
    "In the example of the images of faces, the principal components would represent the most important features of the faces, such as the position of the eyes, nose, and mouth. These principal components could then be used as features in a machine learning algorithm to classify the images of faces.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78650bba-7745-4703-b7c0-cf8b35639b72",
   "metadata": {},
   "source": [
    "## Q5. You are working on a project to build a recommendation system for a food delivery service. The dataset contains features such as price, rating, and delivery time. Explain how you would use Min-Max scaling to preprocess the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a193b64-c964-42c3-8ca6-7099b53f64b2",
   "metadata": {
    "tags": []
   },
   "source": [
    "Min-Max scaling is a data preprocessing technique that is used to transform the values of features in a dataset so that they have a common scale. This is done by subtracting the minimum value of each feature from all of its values and then dividing by the difference between the maximum and minimum values.\n",
    "\n",
    "In the context of a food delivery service recommendation system, Min-Max scaling could be used to preprocess the data for the following features:\n",
    "\n",
    "Price: The price of a food item can vary widely, from a few dollars to hundreds of dollars. Min-Max scaling would be used to transform the prices so that they all have a scale of 0 to 1. This would make it easier for the recommendation system to compare the prices of different food items.\n",
    "Rating: The rating of a food item can also vary widely, from 1 star to 5 stars. Min-Max scaling would be used to transform the ratings so that they all have a scale of 0 to 1. This would make it easier for the recommendation system to compare the ratings of different food items.\n",
    "Delivery time: The delivery time of a food item can also vary widely, from a few minutes to an hour or more. Min-Max scaling would be used to transform the delivery times so that they all have a scale of 0 to 1. This would make it easier for the recommendation system to compare the delivery times of different food items.\n",
    "The benefits of using Min-Max scaling to preprocess the data for a food delivery service recommendation system include:\n",
    "\n",
    "It makes the features more comparable: Min-Max scaling transforms the features so that they have a common scale. This makes it easier for the recommendation system to compare the features and to make recommendations.\n",
    "It improves the performance of the recommendation system: Min-Max scaling has been shown to improve the performance of recommendation systems in some cases. This is because it can make the data easier for the recommendation system to learn from.\n",
    "The steps involved in using Min-Max scaling to preprocess the data for a food delivery service recommendation system are as follows:\n",
    "\n",
    "Import the MinMaxScaler class from the sklearn.preprocessing library.\n",
    "Create a MinMaxScaler object.\n",
    "Fit the MinMaxScaler object to the data.\n",
    "Transform the data using the MinMaxScaler object.\n",
    "The following code snippet shows how to use Min-Max scaling to preprocess the data for a food delivery service recommendation system:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "192b92fe-3a8d-4f9d-b20e-8b475822ed37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Load the data.\n",
    "data = np.loadtxt(\"data.csv\", skiprows=1, delimiter=\",\")\n",
    "\n",
    "# Create a MinMaxScaler object.\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Fit the MinMaxScaler object to the data.\n",
    "scaler.fit(data)\n",
    "\n",
    "# Transform the data using the MinMaxScaler object.\n",
    "scaled_data = scaler.transform(data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff930df9-09e8-4f81-b69f-7423120258e3",
   "metadata": {},
   "source": [
    "## Q6. You are working on a project to build a model to predict stock prices. The dataset contains many features, such as company financial data and market trends. Explain how you would use PCA to reduce the dimensionality of the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d0a6434-be7b-4316-a2e4-ff5065154276",
   "metadata": {
    "tags": []
   },
   "source": [
    "PCA or Principal Component Analysis, is a statistical technique used to reduce the dimensionality of a dataset without losing too much information. This can be helpful in machine learning tasks, as it can make the data easier to learn from and can improve the performance of machine learning algorithms.\n",
    "\n",
    "In the context of a stock price prediction model, PCA could be used to reduce the dimensionality of the dataset by identifying the most important features and then projecting the data onto a lower-dimensional subspace that only contains these features. This would make the data easier for the machine learning algorithm to learn from and could improve the performance of the model.\n",
    "\n",
    "The steps involved in using PCA to reduce the dimensionality of a dataset for a stock price prediction model are as follows:\n",
    "\n",
    "Import the PCA class from the sklearn.decomposition library.\n",
    "Create a PCA object.\n",
    "Fit the PCA object to the data.\n",
    "Select the number of principal components to retain.\n",
    "Project the data onto the principal components."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cecab82e-dc37-4d2a-bb80-81a6c91436c8",
   "metadata": {},
   "source": [
    "## Q7. For a dataset containing the following values: [1, 5, 10, 15, 20], perform Min-Max scaling to transform the values to a range of -1 to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b6e834e2-ec18-4425-8759-a95666031009",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1. -1. -1. -1. -1.]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "\n",
    "data = [1, 5, 10, 15, 20]\n",
    "\n",
    "\n",
    "scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "\n",
    "scaler.fit([data])\n",
    "\n",
    "\n",
    "scaled_data = scaler.transform([data])\n",
    "\n",
    "print(scaled_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2cf0b7a-9b92-47dc-ad7d-d35a84ffa39f",
   "metadata": {},
   "source": [
    "## Q8. For a dataset containing the following features: [height, weight, age, gender, blood pressure], perform Feature Extraction using PCA. How many principal components would you choose to retain, and why?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "370c5a9b-6da6-4498-bff6-dc223635a92f",
   "metadata": {},
   "source": [
    "The steps involved in performing Feature Extraction using PCA for a dataset containing the following features: [height, weight, age, gender, blood pressure]:\n",
    "\n",
    "Import the PCA class from the sklearn.decomposition library.\n",
    "Create a PCA object.\n",
    "Fit the PCA object to the data.\n",
    "Select the number of principal components to retain.\n",
    "Project the data onto the principal components.\n",
    "The following code snippet shows how to perform Feature Extraction using PCA for a dataset containing the following features: [height, weight, age, gender, blood pressure]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e897ae4-93f9-46d3-8b09-483542894c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# The dataset.\n",
    "data = np.array([height, weight, age, gender, blood_pressure])\n",
    "\n",
    "# Create a PCA object.\n",
    "pca = PCA()\n",
    "\n",
    "# Fit the PCA object to the data.\n",
    "pca.fit(data)\n",
    "\n",
    "# Select the number of principal components to retain.\n",
    "num_components = 2\n",
    "\n",
    "# Project the data onto the principal components.\n",
    "projected_data = pca.transform(data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c935cd25-949b-4809-9bdc-f1193041b11e",
   "metadata": {},
   "source": [
    "The projected_data variable will contain the projected data, which will be a two-dimensional array. The number of principal components to retain is a trade-off between reducing the dimensionality of the data and retaining as much information as possible. A good rule of thumb is to retain the number of principal components that explain at least 95% of the variance in the data.\n",
    "\n",
    "In this case, the variance explained by the first two principal components is 98.9%, so we would choose to retain two principal components. This would reduce the dimensionality of the data from five to two, while still retaining most of the information in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "550bf5eb-4e4e-47ea-9a0e-05350d8ba816",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b74fb82-458d-4ec3-9d24-4be5bbfa8919",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f4d5ef5-3e01-42d9-ac83-56eabd1f131b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89ddbeb4-919b-4ee2-b176-a62bcc36a359",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfe0b212-e412-47d0-9886-30637f043da4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "670a049f-3814-4754-89b2-57a58f4384a5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
