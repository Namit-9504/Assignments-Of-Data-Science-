{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "364ebb49-7830-4378-ae38-afa5fc26b17d",
   "metadata": {},
   "source": [
    "## Q1. A company conducted a survey of its employees and found that 70% of the employees use the company's health insurance plan, while 40% of the employees who use the plan are smokers. What is the probability that an employee is a smoker given that he/she uses the health insurance plan?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a186f207-df57-4e51-a57e-82fd6e86fc91",
   "metadata": {},
   "source": [
    "To find the probability that an employee is a smoker given that he/she uses the health insurance plan, we can use Bayes' theorem. Let's define the events:\n",
    "\n",
    "A: Employee uses the health insurance plan.\n",
    "B: Employee is a smoker.\n",
    "\n",
    "We are given the following probabilities:\n",
    "\n",
    "1. \\( P(A) \\): Probability that an employee uses the health insurance plan = 0.70 (70%)\n",
    "2. \\( P(B|A) \\): Probability that an employee is a smoker given that he/she uses the health insurance plan = 0.40 (40%)\n",
    "\n",
    "We want to find \\( P(B|A) \\), the probability that an employee is a smoker given that he/she uses the health insurance plan.\n",
    "\n",
    "Bayes' theorem states:\n",
    "\n",
    "\\[ P(B|A) = \\frac{P(A|B) \\cdot P(B)}{P(A)} \\]\n",
    "\n",
    "Where:\n",
    "- \\( P(B|A) \\) is the probability of B (smoker) given A (uses health insurance plan).\n",
    "- \\( P(A|B) \\) is the probability of A (uses health insurance plan) given B (smoker). This is what we want to find.\n",
    "- \\( P(B) \\) is the probability of B (smoker).\n",
    "- \\( P(A) \\) is the probability of A (uses health insurance plan).\n",
    "\n",
    "We know \\( P(A|B) = 0.40 \\) (40% of employees who use the plan are smokers) and \\( P(A) = 0.70 \\) (70% of employees use the plan). We need to calculate \\( P(B) \\), the probability of an employee being a smoker.\n",
    "\n",
    "To calculate \\( P(B) \\), we can use the Law of Total Probability:\n",
    "\n",
    "\\[ P(B) = P(B|A) \\cdot P(A) + P(B|\\neg A) \\cdot P(\\neg A) \\]\n",
    "\n",
    "Where:\n",
    "- \\( P(B|\\neg A) \\) is the probability of B (smoker) given not A (does not use health insurance plan). This information is not given, so we'll assume it to be independent of health insurance usage and set it to a value (e.g., 0.20).\n",
    "- \\( P(\\neg A) \\) is the probability of not A (does not use health insurance plan) = 1 - \\( P(A) \\) = 1 - 0.70 = 0.30.\n",
    "\n",
    "Now, let's calculate \\( P(B) \\) and then use Bayes' theorem to find \\( P(B|A) \\):\n",
    "\n",
    "```python\n",
    "# Given probabilities\n",
    "P_A = 0.70\n",
    "P_B_given_A = 0.40\n",
    "P_B_given_not_A = 0.20\n",
    "P_not_A = 1 - P_A\n",
    "\n",
    "# Calculate P(B) using the Law of Total Probability\n",
    "P_B = P_B_given_A * P_A + P_B_given_not_A * P_not_A\n",
    "\n",
    "# Use Bayes' theorem to find P(B|A)\n",
    "P_B_given_A = (P_B_given_A * P_A) / P_B\n",
    "\n",
    "print(\"Probability that an employee is a smoker given that he/she uses the health insurance plan:\", P_B_given_A)\n",
    "```\n",
    "\n",
    "After running the above code, you will get the probability that an employee is a smoker given that he/she uses the health insurance plan."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0090ae07-4ac8-4a99-a0d3-ab15e137c1b2",
   "metadata": {},
   "source": [
    "## Q2. What is the difference between Bernoulli Naive Bayes and Multinomial Naive Bayes?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0814d988-e9b0-4346-84d6-955e3883e5f3",
   "metadata": {},
   "source": [
    "Bernoulli Naive Bayes and Multinomial Naive Bayes are both variants of the Naive Bayes classifier, but they differ in the type of data they can handle and the way they model the features. The key differences between them are as follows:\n",
    "\n",
    "**1. Data Representation:**\n",
    "- **Bernoulli Naive Bayes:** It is designed for binary data, where each feature can take on only two values, typically 0 or 1. It assumes that each feature is a binary variable and models the presence or absence of a feature in a document.\n",
    "- **Multinomial Naive Bayes:** It is suitable for discrete data, where each feature represents the count or frequency of occurrences of a specific event. It works with integer feature counts and can handle multiple categories or classes for each feature.\n",
    "\n",
    "**2. Feature Model:**\n",
    "- **Bernoulli Naive Bayes:** In Bernoulli Naive Bayes, each feature is treated as a binary variable, and the probability of each feature occurring is modeled using a Bernoulli distribution. It focuses on whether a feature is present (1) or absent (0) in a document.\n",
    "- **Multinomial Naive Bayes:** In Multinomial Naive Bayes, the probability of each feature occurring is modeled using a multinomial distribution. It considers the counts or frequencies of different categories or events for each feature.\n",
    "\n",
    "**3. Handling of Absent Features:**\n",
    "- **Bernoulli Naive Bayes:** It takes into account the absence of features (i.e., features that are not present) and incorporates them into the classification process.\n",
    "- **Multinomial Naive Bayes:** It typically ignores the absence of features and only focuses on the counts or frequencies of the features that are present.\n",
    "\n",
    "**4. Use Cases:**\n",
    "- **Bernoulli Naive Bayes:** It is commonly used in text classification tasks where the features represent the presence or absence of words in a document, such as spam detection or sentiment analysis.\n",
    "- **Multinomial Naive Bayes:** It is widely used in text classification tasks where the features represent the counts or frequencies of words or tokens in a document, such as document categorization or topic modeling.\n",
    "\n",
    "In summary, the choice between Bernoulli Naive Bayes and Multinomial Naive Bayes depends on the type of data and the representation of features in the dataset. If the features are binary (presence/absence), Bernoulli Naive Bayes is more appropriate. If the features are discrete counts or frequencies, Multinomial Naive Bayes is a better choice. Both classifiers are simple and fast, making them popular choices for text and document classification tasks in natural language processing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0e8caf2-e989-45f6-9b82-738a5dca2694",
   "metadata": {},
   "source": [
    "## Q3. How does Bernoulli Naive Bayes handle missing values?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "306a9c80-f1b1-49c4-a89d-833758999f3b",
   "metadata": {},
   "source": [
    "Bernoulli Naive Bayes is designed to handle binary data, where each feature can take on only two values, typically represented as 0 and 1. When dealing with missing values in Bernoulli Naive Bayes, the approach taken depends on how the missing values are encoded or treated in the data.\n",
    "\n",
    "There are two common approaches to handle missing values in Bernoulli Naive Bayes:\n",
    "\n",
    "**1. Ignoring Missing Values:**\n",
    "One approach is to simply ignore the missing values and exclude those instances from the analysis. In this case, the instances with missing values are not used during training the model. During prediction, if a new instance contains missing values for some features, those features are ignored, and the model is applied only to the non-missing features. This is the simplest approach, but it may lead to a loss of information if there is valuable data in the missing features.\n",
    "\n",
    "**2. Encoding Missing Values:**\n",
    "Another approach is to encode missing values as a separate category or value, distinct from 0 and 1. This way, missing values are treated as a third category in the binary feature representation. For example, if the original binary features are represented as 0 and 1, the missing values can be encoded as -1 (or any other value that does not represent 0 or 1). During training, the model will consider the missing values as a separate category and learn the probability of this category being associated with the target class.\n",
    "\n",
    "When using the encoding approach, it is important to be mindful of how the missing values are handled during data preprocessing and model training. One common practice is to use the training data to estimate the probability of the missing value category being associated with each class. During prediction, if a new instance contains missing values, the model will use these estimated probabilities to classify the instance, taking into account the missing value category.\n",
    "\n",
    "In summary, Bernoulli Naive Bayes can handle missing values by either ignoring them or encoding them as a separate category. The choice of approach depends on the specific characteristics of the data and the impact of missing values on the analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a3eb225-4021-4853-9b8f-96a9ee9eaf18",
   "metadata": {},
   "source": [
    "## Q4. Can Gaussian Naive Bayes be used for multi-class classification?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "258cf238-a82e-4c71-91d5-ae9da577e2aa",
   "metadata": {},
   "source": [
    "Yes, Gaussian Naive Bayes can be used for multi-class classification. Gaussian Naive Bayes is an extension of the Naive Bayes algorithm that is suitable for continuous or real-valued features. It assumes that each feature follows a Gaussian (normal) distribution within each class.\n",
    "\n",
    "For multi-class classification, where there are more than two classes to predict, Gaussian Naive Bayes can handle the problem by applying the algorithm to each class separately and then making a decision based on the probabilities obtained for each class.\n",
    "\n",
    "Here's how Gaussian Naive Bayes works for multi-class classification:\n",
    "\n",
    "1. **Training Phase:**\n",
    "   - For each class, calculate the mean and variance of each feature using the training data instances belonging to that class.\n",
    "   - This involves calculating the mean and variance for each feature for each class, assuming they are normally distributed.\n",
    "\n",
    "2. **Prediction Phase:**\n",
    "   - Given a new instance with continuous feature values, calculate the likelihood of each feature's value belonging to each class using the Gaussian probability density function.\n",
    "   - Combine the individual feature probabilities for each class using Bayes' theorem to get the posterior probability for each class.\n",
    "   - Assign the new instance to the class with the highest posterior probability.\n",
    "\n",
    "The decision rule for multi-class classification using Gaussian Naive Bayes is to select the class with the highest posterior probability given the observed feature values. The posterior probability is calculated as the product of the prior probability of the class and the likelihoods of the individual features, assuming they are independent.\n",
    "\n",
    "While Gaussian Naive Bayes can be used for multi-class classification, it may not always be the best choice for highly complex or high-dimensional data. In such cases, other algorithms like Multinomial Naive Bayes, Decision Trees, Random Forests, or Support Vector Machines may offer better performance. However, Gaussian Naive Bayes remains a simple and computationally efficient algorithm for multi-class classification tasks, especially when dealing with continuous features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e7d1869-6a02-4238-884a-d808b65e7ff8",
   "metadata": {},
   "source": [
    "## Q5. Assignment:\n",
    "\n",
    "Data preparation:\n",
    "\n",
    "Download the \"Spambase Data Set\" from the UCI Machine Learning Repository (https://archive.ics.uci.edu/ml/\n",
    "datasets/Spambase). This dataset contains email messages, where the goal is to predict whether a message\n",
    "is spam or not based on several input features.\n",
    "\n",
    "Implementation:\n",
    "\n",
    "Implement Bernoulli Naive Bayes, Multinomial Naive Bayes, and Gaussian Naive Bayes classifiers using the\n",
    "scikit-learn library in Python. Use 10-fold cross-validation to evaluate the performance of each classifier on the\n",
    "dataset. You should use the default hyperparameters for each classifier.\n",
    "\n",
    "Results:\n",
    "\n",
    "- Report the following performance metrics for each classifier:\n",
    " -Accuracy\n",
    "- Precision\n",
    "- Recall\n",
    "- F1 score\n",
    "\n",
    "Discussion:\n",
    "\n",
    "Discuss the results you obtained. Which variant of Naive Bayes performed the best? Why do you think that is\n",
    "the case? Are there any limitations of Naive Bayes that you observed?\n",
    "\n",
    "Conclusion:\n",
    "\n",
    "\n",
    "Summarise your findings and provide some suggestions for future work.\n",
    "\n",
    "\n",
    "Note: This dataset contains a binary classification problem with multiple features. The dataset is\n",
    "relatively small, but it can be used to demonstrate the performance of the different variants of Naive\n",
    "Bayes on a real-world problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f150e2a-d56a-4c19-9891-442fdfe3c068",
   "metadata": {},
   "source": [
    "### Data preparation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ec3ce636-f508-4ec2-a315-58c12b3447f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open ('spambase/spambase.names','r') as f:\n",
    "    a=f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "702f3e44-004b-4166-a864-089424525a6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| SPAM E-MAIL DATABASE ATTRIBUTES (in .names format)\n",
      "|\n",
      "| 48 continuous real [0,100] attributes of type word_freq_WORD \n",
      "| = percentage of words in the e-mail that match WORD,\n",
      "| i.e. 100 * (number of times the WORD appears in the e-mail) / \n",
      "| total number of words in e-mail.  A \"word\" in this case is any \n",
      "| string of alphanumeric characters bounded by non-alphanumeric \n",
      "| characters or end-of-string.\n",
      "|\n",
      "| 6 continuous real [0,100] attributes of type char_freq_CHAR\n",
      "| = percentage of characters in the e-mail that match CHAR,\n",
      "| i.e. 100 * (number of CHAR occurences) / total characters in e-mail\n",
      "|\n",
      "| 1 continuous real [1,...] attribute of type capital_run_length_average\n",
      "| = average length of uninterrupted sequences of capital letters\n",
      "|\n",
      "| 1 continuous integer [1,...] attribute of type capital_run_length_longest\n",
      "| = length of longest uninterrupted sequence of capital letters\n",
      "|\n",
      "| 1 continuous integer [1,...] attribute of type capital_run_length_total\n",
      "| = sum of length of uninterrupted sequences of capital letters\n",
      "| = total number of capital letters in the e-mail\n",
      "|\n",
      "| 1 nominal {0,1} class attribute of type spam\n",
      "| = denotes whether the e-mail was considered spam (1) or not (0), \n",
      "| i.e. unsolicited commercial e-mail.  \n",
      "|\n",
      "| For more information, see file 'spambase.DOCUMENTATION' at the\n",
      "| UCI Machine Learning Repository: http://www.ics.uci.edu/~mlearn/MLRepository.html\n",
      "\n",
      "\n",
      "1, 0.    | spam, non-spam classes\n",
      "\n",
      "word_freq_make:         continuous.\n",
      "word_freq_address:      continuous.\n",
      "word_freq_all:          continuous.\n",
      "word_freq_3d:           continuous.\n",
      "word_freq_our:          continuous.\n",
      "word_freq_over:         continuous.\n",
      "word_freq_remove:       continuous.\n",
      "word_freq_internet:     continuous.\n",
      "word_freq_order:        continuous.\n",
      "word_freq_mail:         continuous.\n",
      "word_freq_receive:      continuous.\n",
      "word_freq_will:         continuous.\n",
      "word_freq_people:       continuous.\n",
      "word_freq_report:       continuous.\n",
      "word_freq_addresses:    continuous.\n",
      "word_freq_free:         continuous.\n",
      "word_freq_business:     continuous.\n",
      "word_freq_email:        continuous.\n",
      "word_freq_you:          continuous.\n",
      "word_freq_credit:       continuous.\n",
      "word_freq_your:         continuous.\n",
      "word_freq_font:         continuous.\n",
      "word_freq_000:          continuous.\n",
      "word_freq_money:        continuous.\n",
      "word_freq_hp:           continuous.\n",
      "word_freq_hpl:          continuous.\n",
      "word_freq_george:       continuous.\n",
      "word_freq_650:          continuous.\n",
      "word_freq_lab:          continuous.\n",
      "word_freq_labs:         continuous.\n",
      "word_freq_telnet:       continuous.\n",
      "word_freq_857:          continuous.\n",
      "word_freq_data:         continuous.\n",
      "word_freq_415:          continuous.\n",
      "word_freq_85:           continuous.\n",
      "word_freq_technology:   continuous.\n",
      "word_freq_1999:         continuous.\n",
      "word_freq_parts:        continuous.\n",
      "word_freq_pm:           continuous.\n",
      "word_freq_direct:       continuous.\n",
      "word_freq_cs:           continuous.\n",
      "word_freq_meeting:      continuous.\n",
      "word_freq_original:     continuous.\n",
      "word_freq_project:      continuous.\n",
      "word_freq_re:           continuous.\n",
      "word_freq_edu:          continuous.\n",
      "word_freq_table:        continuous.\n",
      "word_freq_conference:   continuous.\n",
      "char_freq_;:            continuous.\n",
      "char_freq_(:            continuous.\n",
      "char_freq_[:            continuous.\n",
      "char_freq_!:            continuous.\n",
      "char_freq_$:            continuous.\n",
      "char_freq_#:            continuous.\n",
      "capital_run_length_average: continuous.\n",
      "capital_run_length_longest: continuous.\n",
      "capital_run_length_total:   continuous.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9501f99e-c646-48aa-9baa-a248e7fd248c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open ('spambase/spambase.DOCUMENTATION', 'r') as f1:\n",
    "    b=f1.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e1db3b54-4074-4c00-a970-6976afb9acb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Title:  SPAM E-mail Database\n",
      "\n",
      "2. Sources:\n",
      "   (a) Creators: Mark Hopkins, Erik Reeber, George Forman, Jaap Suermondt\n",
      "        Hewlett-Packard Labs, 1501 Page Mill Rd., Palo Alto, CA 94304\n",
      "   (b) Donor: George Forman (gforman at nospam hpl.hp.com)  650-857-7835\n",
      "   (c) Generated: June-July 1999\n",
      "\n",
      "3. Past Usage:\n",
      "   (a) Hewlett-Packard Internal-only Technical Report. External forthcoming.\n",
      "   (b) Determine whether a given email is spam or not.\n",
      "   (c) ~7% misclassification error.\n",
      "       False positives (marking good mail as spam) are very undesirable.\n",
      "       If we insist on zero false positives in the training/testing set,\n",
      "       20-25% of the spam passed through the filter.\n",
      "\n",
      "4. Relevant Information:\n",
      "        The \"spam\" concept is diverse: advertisements for products/web\n",
      "        sites, make money fast schemes, chain letters, pornography...\n",
      "\tOur collection of spam e-mails came from our postmaster and \n",
      "\tindividuals who had filed spam.  Our collection of non-spam \n",
      "\te-mails came from filed work and personal e-mails, and hence\n",
      "\tthe word 'george' and the area code '650' are indicators of \n",
      "\tnon-spam.  These are useful when constructing a personalized \n",
      "\tspam filter.  One would either have to blind such non-spam \n",
      "\tindicators or get a very wide collection of non-spam to \n",
      "\tgenerate a general purpose spam filter.\n",
      "\n",
      "        For background on spam:\n",
      "        Cranor, Lorrie F., LaMacchia, Brian A.  Spam! \n",
      "        Communications of the ACM, 41(8):74-83, 1998.\n",
      "\n",
      "5. Number of Instances: 4601 (1813 Spam = 39.4%)\n",
      "\n",
      "6. Number of Attributes: 58 (57 continuous, 1 nominal class label)\n",
      "\n",
      "7. Attribute Information:\n",
      "The last column of 'spambase.data' denotes whether the e-mail was \n",
      "considered spam (1) or not (0), i.e. unsolicited commercial e-mail.  \n",
      "Most of the attributes indicate whether a particular word or\n",
      "character was frequently occuring in the e-mail.  The run-length\n",
      "attributes (55-57) measure the length of sequences of consecutive \n",
      "capital letters.  For the statistical measures of each attribute, \n",
      "see the end of this file.  Here are the definitions of the attributes:\n",
      "\n",
      "48 continuous real [0,100] attributes of type word_freq_WORD \n",
      "= percentage of words in the e-mail that match WORD,\n",
      "i.e. 100 * (number of times the WORD appears in the e-mail) / \n",
      "total number of words in e-mail.  A \"word\" in this case is any \n",
      "string of alphanumeric characters bounded by non-alphanumeric \n",
      "characters or end-of-string.\n",
      "\n",
      "6 continuous real [0,100] attributes of type char_freq_CHAR\n",
      "= percentage of characters in the e-mail that match CHAR,\n",
      "i.e. 100 * (number of CHAR occurences) / total characters in e-mail\n",
      "\n",
      "1 continuous real [1,...] attribute of type capital_run_length_average\n",
      "= average length of uninterrupted sequences of capital letters\n",
      "\n",
      "1 continuous integer [1,...] attribute of type capital_run_length_longest\n",
      "= length of longest uninterrupted sequence of capital letters\n",
      "\n",
      "1 continuous integer [1,...] attribute of type capital_run_length_total\n",
      "= sum of length of uninterrupted sequences of capital letters\n",
      "= total number of capital letters in the e-mail\n",
      "\n",
      "1 nominal {0,1} class attribute of type spam\n",
      "= denotes whether the e-mail was considered spam (1) or not (0), \n",
      "i.e. unsolicited commercial e-mail.  \n",
      "\n",
      "\n",
      "8. Missing Attribute Values: None\n",
      "\n",
      "9. Class Distribution:\n",
      "\tSpam\t  1813  (39.4%)\n",
      "\tNon-Spam  2788  (60.6%)\n",
      "\n",
      "\n",
      "Attribute Statistics:\n",
      "   Min: Max:   Average:  Std.Dev: Coeff.Var_%: \n",
      "1  0    4.54   0.10455   0.30536  292          \n",
      "2  0    14.28  0.21301   1.2906   606          \n",
      "3  0    5.1    0.28066   0.50414  180          \n",
      "4  0    42.81  0.065425  1.3952   2130         \n",
      "5  0    10     0.31222   0.67251  215          \n",
      "6  0    5.88   0.095901  0.27382  286          \n",
      "7  0    7.27   0.11421   0.39144  343          \n",
      "8  0    11.11  0.10529   0.40107  381          \n",
      "9  0    5.26   0.090067  0.27862  309          \n",
      "10 0    18.18  0.23941   0.64476  269          \n",
      "11 0    2.61   0.059824  0.20154  337          \n",
      "12 0    9.67   0.5417    0.8617   159          \n",
      "13 0    5.55   0.09393   0.30104  320          \n",
      "14 0    10     0.058626  0.33518  572          \n",
      "15 0    4.41   0.049205  0.25884  526          \n",
      "16 0    20     0.24885   0.82579  332          \n",
      "17 0    7.14   0.14259   0.44406  311          \n",
      "18 0    9.09   0.18474   0.53112  287          \n",
      "19 0    18.75  1.6621    1.7755   107          \n",
      "20 0    18.18  0.085577  0.50977  596          \n",
      "21 0    11.11  0.80976   1.2008   148          \n",
      "22 0    17.1   0.1212    1.0258   846          \n",
      "23 0    5.45   0.10165   0.35029  345          \n",
      "24 0    12.5   0.094269  0.44264  470          \n",
      "25 0    20.83  0.5495    1.6713   304          \n",
      "26 0    16.66  0.26538   0.88696  334          \n",
      "27 0    33.33  0.7673    3.3673   439          \n",
      "28 0    9.09   0.12484   0.53858  431          \n",
      "29 0    14.28  0.098915  0.59333  600          \n",
      "30 0    5.88   0.10285   0.45668  444          \n",
      "31 0    12.5   0.064753  0.40339  623          \n",
      "32 0    4.76   0.047048  0.32856  698          \n",
      "33 0    18.18  0.097229  0.55591  572          \n",
      "34 0    4.76   0.047835  0.32945  689          \n",
      "35 0    20     0.10541   0.53226  505          \n",
      "36 0    7.69   0.097477  0.40262  413          \n",
      "37 0    6.89   0.13695   0.42345  309          \n",
      "38 0    8.33   0.013201  0.22065  1670         \n",
      "39 0    11.11  0.078629  0.43467  553          \n",
      "40 0    4.76   0.064834  0.34992  540          \n",
      "41 0    7.14   0.043667  0.3612   827          \n",
      "42 0    14.28  0.13234   0.76682  579          \n",
      "43 0    3.57   0.046099  0.22381  486          \n",
      "44 0    20     0.079196  0.62198  785          \n",
      "45 0    21.42  0.30122   1.0117   336          \n",
      "46 0    22.05  0.17982   0.91112  507          \n",
      "47 0    2.17   0.0054445 0.076274 1400         \n",
      "48 0    10     0.031869  0.28573  897          \n",
      "49 0    4.385  0.038575  0.24347  631          \n",
      "50 0    9.752  0.13903   0.27036  194          \n",
      "51 0    4.081  0.016976  0.10939  644          \n",
      "52 0    32.478 0.26907   0.81567  303          \n",
      "53 0    6.003  0.075811  0.24588  324          \n",
      "54 0    19.829 0.044238  0.42934  971          \n",
      "55 1    1102.5 5.1915    31.729   611          \n",
      "56 1    9989   52.173    194.89   374          \n",
      "57 1    15841  283.29    606.35   214          \n",
      "58 0    1      0.39404   0.4887   124          \n",
      "\n",
      "\n",
      "This file: 'spambase.DOCUMENTATION' at the UCI Machine Learning Repository\n",
      "http://www.ics.uci.edu/~mlearn/MLRepository.html\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b32e944d-d3ca-40dd-b308-03915d59fd60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>48</th>\n",
       "      <th>49</th>\n",
       "      <th>50</th>\n",
       "      <th>51</th>\n",
       "      <th>52</th>\n",
       "      <th>53</th>\n",
       "      <th>54</th>\n",
       "      <th>55</th>\n",
       "      <th>56</th>\n",
       "      <th>57</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.778</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.756</td>\n",
       "      <td>61</td>\n",
       "      <td>278</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.21</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.94</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.132</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.372</td>\n",
       "      <td>0.180</td>\n",
       "      <td>0.048</td>\n",
       "      <td>5.114</td>\n",
       "      <td>101</td>\n",
       "      <td>1028</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.06</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.23</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.25</td>\n",
       "      <td>...</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.143</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.276</td>\n",
       "      <td>0.184</td>\n",
       "      <td>0.010</td>\n",
       "      <td>9.821</td>\n",
       "      <td>485</td>\n",
       "      <td>2259</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.537</td>\n",
       "      <td>40</td>\n",
       "      <td>191</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.537</td>\n",
       "      <td>40</td>\n",
       "      <td>191</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 58 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     0     1     2    3     4     5     6     7     8     9   ...    48  \\\n",
       "0  0.00  0.64  0.64  0.0  0.32  0.00  0.00  0.00  0.00  0.00  ...  0.00   \n",
       "1  0.21  0.28  0.50  0.0  0.14  0.28  0.21  0.07  0.00  0.94  ...  0.00   \n",
       "2  0.06  0.00  0.71  0.0  1.23  0.19  0.19  0.12  0.64  0.25  ...  0.01   \n",
       "3  0.00  0.00  0.00  0.0  0.63  0.00  0.31  0.63  0.31  0.63  ...  0.00   \n",
       "4  0.00  0.00  0.00  0.0  0.63  0.00  0.31  0.63  0.31  0.63  ...  0.00   \n",
       "\n",
       "      49   50     51     52     53     54   55    56  57  \n",
       "0  0.000  0.0  0.778  0.000  0.000  3.756   61   278   1  \n",
       "1  0.132  0.0  0.372  0.180  0.048  5.114  101  1028   1  \n",
       "2  0.143  0.0  0.276  0.184  0.010  9.821  485  2259   1  \n",
       "3  0.137  0.0  0.137  0.000  0.000  3.537   40   191   1  \n",
       "4  0.135  0.0  0.135  0.000  0.000  3.537   40   191   1  \n",
       "\n",
       "[5 rows x 58 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df= pd.read_csv('spambase/spambase.data',header=None)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8c8e1429-ca3c-4e26-948c-d7abf9ee44be",
   "metadata": {},
   "outputs": [],
   "source": [
    "features=[]\n",
    "for i in range(df.shape[1]) :\n",
    "    if i!=57:\n",
    "        f='f'+str(i+1)\n",
    "        features.append(f)\n",
    "    else:\n",
    "        fs='target'\n",
    "        features.append(fs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ea0dae5e-fe41-4725-986d-5bef7097d149",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns=features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ec94ee8b-2dc0-4324-9bfc-0dc489e92a4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>f1</th>\n",
       "      <th>f2</th>\n",
       "      <th>f3</th>\n",
       "      <th>f4</th>\n",
       "      <th>f5</th>\n",
       "      <th>f6</th>\n",
       "      <th>f7</th>\n",
       "      <th>f8</th>\n",
       "      <th>f9</th>\n",
       "      <th>f10</th>\n",
       "      <th>...</th>\n",
       "      <th>f49</th>\n",
       "      <th>f50</th>\n",
       "      <th>f51</th>\n",
       "      <th>f52</th>\n",
       "      <th>f53</th>\n",
       "      <th>f54</th>\n",
       "      <th>f55</th>\n",
       "      <th>f56</th>\n",
       "      <th>f57</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.778</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.756</td>\n",
       "      <td>61</td>\n",
       "      <td>278</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.21</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.94</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.132</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.372</td>\n",
       "      <td>0.180</td>\n",
       "      <td>0.048</td>\n",
       "      <td>5.114</td>\n",
       "      <td>101</td>\n",
       "      <td>1028</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.06</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.23</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.25</td>\n",
       "      <td>...</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.143</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.276</td>\n",
       "      <td>0.184</td>\n",
       "      <td>0.010</td>\n",
       "      <td>9.821</td>\n",
       "      <td>485</td>\n",
       "      <td>2259</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.537</td>\n",
       "      <td>40</td>\n",
       "      <td>191</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.537</td>\n",
       "      <td>40</td>\n",
       "      <td>191</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 58 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     f1    f2    f3   f4    f5    f6    f7    f8    f9   f10  ...   f49  \\\n",
       "0  0.00  0.64  0.64  0.0  0.32  0.00  0.00  0.00  0.00  0.00  ...  0.00   \n",
       "1  0.21  0.28  0.50  0.0  0.14  0.28  0.21  0.07  0.00  0.94  ...  0.00   \n",
       "2  0.06  0.00  0.71  0.0  1.23  0.19  0.19  0.12  0.64  0.25  ...  0.01   \n",
       "3  0.00  0.00  0.00  0.0  0.63  0.00  0.31  0.63  0.31  0.63  ...  0.00   \n",
       "4  0.00  0.00  0.00  0.0  0.63  0.00  0.31  0.63  0.31  0.63  ...  0.00   \n",
       "\n",
       "     f50  f51    f52    f53    f54    f55  f56   f57  target  \n",
       "0  0.000  0.0  0.778  0.000  0.000  3.756   61   278       1  \n",
       "1  0.132  0.0  0.372  0.180  0.048  5.114  101  1028       1  \n",
       "2  0.143  0.0  0.276  0.184  0.010  9.821  485  2259       1  \n",
       "3  0.137  0.0  0.137  0.000  0.000  3.537   40   191       1  \n",
       "4  0.135  0.0  0.135  0.000  0.000  3.537   40   191       1  \n",
       "\n",
       "[5 rows x 58 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d70945db-1ce5-4747-aa9c-943003b40ea3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "f1        0\n",
       "f2        0\n",
       "f3        0\n",
       "f4        0\n",
       "f5        0\n",
       "f6        0\n",
       "f7        0\n",
       "f8        0\n",
       "f9        0\n",
       "f10       0\n",
       "f11       0\n",
       "f12       0\n",
       "f13       0\n",
       "f14       0\n",
       "f15       0\n",
       "f16       0\n",
       "f17       0\n",
       "f18       0\n",
       "f19       0\n",
       "f20       0\n",
       "f21       0\n",
       "f22       0\n",
       "f23       0\n",
       "f24       0\n",
       "f25       0\n",
       "f26       0\n",
       "f27       0\n",
       "f28       0\n",
       "f29       0\n",
       "f30       0\n",
       "f31       0\n",
       "f32       0\n",
       "f33       0\n",
       "f34       0\n",
       "f35       0\n",
       "f36       0\n",
       "f37       0\n",
       "f38       0\n",
       "f39       0\n",
       "f40       0\n",
       "f41       0\n",
       "f42       0\n",
       "f43       0\n",
       "f44       0\n",
       "f45       0\n",
       "f46       0\n",
       "f47       0\n",
       "f48       0\n",
       "f49       0\n",
       "f50       0\n",
       "f51       0\n",
       "f52       0\n",
       "f53       0\n",
       "f54       0\n",
       "f55       0\n",
       "f56       0\n",
       "f57       0\n",
       "target    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "794c884a-1ac5-47d3-a69d-99613c10d5f0",
   "metadata": {},
   "source": [
    "### Implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "edc32511-2b0e-404f-bf95-cc4f6a3a96f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Segreating independent and dependate features\n",
    "X=df.drop(columns=\"target\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0bafb433-b287-48a6-8043-0ab8d533043f",
   "metadata": {},
   "outputs": [],
   "source": [
    "y=df['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b719e15a-3272-4ba2-a28a-a1eb9af2723c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.25,random_state=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "166b0403-dba8-4f52-be37-71fef718cb23",
   "metadata": {},
   "source": [
    "#### Bernoulli Naive Bayes,`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9be318e8-ec5e-4d58-b263-41313c5bee0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-3 {color: black;background-color: white;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>BernoulliNB()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" checked><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">BernoulliNB</label><div class=\"sk-toggleable__content\"><pre>BernoulliNB()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "BernoulliNB()"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB\n",
    "ber=BernoulliNB()\n",
    "ber.fit(X_train,y_train.values.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2c27ee23-acab-4576-9da4-fbe82a58db0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.88372093, 0.84644195, 0.81509434, 0.85384615, 0.824     ,\n",
       "       0.81509434, 0.86363636, 0.86590038, 0.88461538, 0.824     ])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "skf =  StratifiedKFold(n_splits=10,shuffle=True,random_state=42)\n",
    "from sklearn.model_selection import cross_val_score\n",
    "scores_bnb = cross_val_score(BernoulliNB(),X_train,y_train.values.flatten(),cv=skf,scoring='f1')\n",
    "scores_bnb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4f677f4c-b1b3-44d8-82f4-3d2d44bde6ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for BernoulliNB :\n",
      "Mean 10 fold cross validation f1 score is : 0.8476\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "mean_score_bnb = np.mean(scores_bnb)\n",
    "print('Results for BernoulliNB :')\n",
    "print(f'Mean 10 fold cross validation f1 score is : {mean_score_bnb:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "118f58e2-800d-4ef4-8b28-55163a628bf3",
   "metadata": {},
   "source": [
    "#### Multinomial Naive Bayes,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2610797c-9de0-4f85-9a91-86a43a1881ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-4 {color: black;background-color: white;}#sk-container-id-4 pre{padding: 0;}#sk-container-id-4 div.sk-toggleable {background-color: white;}#sk-container-id-4 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-4 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-4 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-4 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-4 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-4 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-4 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-4 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-4 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-4 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-4 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-4 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-4 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-4 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-4 div.sk-item {position: relative;z-index: 1;}#sk-container-id-4 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-4 div.sk-item::before, #sk-container-id-4 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-4 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-4 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-4 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-4 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-4 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-4 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-4 div.sk-label-container {text-align: center;}#sk-container-id-4 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-4 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-4\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MultinomialNB()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" checked><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MultinomialNB</label><div class=\"sk-toggleable__content\"><pre>MultinomialNB()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "MultinomialNB()"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "mul=MultinomialNB()\n",
    "mul.fit(X_train,y_train.values.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "31a45b7b-9fbe-4652-a4c2-d353494752ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.81081081, 0.6770428 , 0.72324723, 0.74264706, 0.76534296,\n",
       "       0.73356401, 0.68965517, 0.78700361, 0.72030651, 0.72289157])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "stk=StratifiedKFold(n_splits=10,shuffle=True,random_state=42)\n",
    "from sklearn.model_selection import cross_val_score\n",
    "scores_mnb=cross_val_score(mul,X_train,y_train.values.flatten(),scoring='f1',cv=stk)\n",
    "scores_mnb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9f839c8d-d365-4f89-8ae6-7fb085691f34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for MultinomialNB :\n",
      "Mean 10 fold cross validation f1 score is : 0.7373\n"
     ]
    }
   ],
   "source": [
    "mean_score_mnb = np.mean(scores_mnb)\n",
    "print('Results for MultinomialNB :')\n",
    "print(f'Mean 10 fold cross validation f1 score is : {mean_score_mnb:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc7a2de4-5d28-4338-843f-386633f526ba",
   "metadata": {},
   "source": [
    "#### Gaussian Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "617d8d3d-728f-4243-b60f-4173a58dd907",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-5 {color: black;background-color: white;}#sk-container-id-5 pre{padding: 0;}#sk-container-id-5 div.sk-toggleable {background-color: white;}#sk-container-id-5 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-5 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-5 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-5 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-5 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-5 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-5 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-5 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-5 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-5 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-5 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-5 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-5 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-5 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-5 div.sk-item {position: relative;z-index: 1;}#sk-container-id-5 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-5 div.sk-item::before, #sk-container-id-5 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-5 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-5 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-5 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-5 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-5 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-5 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-5 div.sk-label-container {text-align: center;}#sk-container-id-5 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-5 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-5\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GaussianNB()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" checked><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GaussianNB</label><div class=\"sk-toggleable__content\"><pre>GaussianNB()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "GaussianNB()"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "gnb=GaussianNB()\n",
    "gnb.fit(X_train,y_train.values.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "4061a5a4-2d81-4365-9f86-d235e24924a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.81449275, 0.82318841, 0.84057971, 0.82318841, 0.83768116])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score,StratifiedKFold\n",
    "skc=StratifiedKFold(n_splits=10,shuffle=True,random_state=42)\n",
    "score_gnb=cross_val_score(gnb,X_train,y_train.values.flatten())\n",
    "score_gnb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "3d4b246d-3db6-4c64-a32c-e69d6eaeec00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for Gaussian Naive Bayes\n",
      "Mean 10 fold cross validation f1 score is : 0.8278\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "mean_score_gnb = np.mean(score_gnb)\n",
    "print('Results for Gaussian Naive Bayes')\n",
    "print(f'Mean 10 fold cross validation f1 score is : {mean_score_gnb:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5ade0ed-1d8e-4da5-a455-f6a61c5223c4",
   "metadata": {},
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "8588c7c9-33c9-4382-8ef1-ff672aca8658",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to store all above metrics\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "def evaluate_model(x,y,model):\n",
    "    ypred = model.predict(x)\n",
    "    acc = accuracy_score(y,ypred)\n",
    "    pre = precision_score(y,ypred)\n",
    "    rec = recall_score(y,ypred)\n",
    "    f1 = f1_score(y,ypred)\n",
    "    print(f'Accuracy  : {acc:.4f}')\n",
    "    print(f'Precision : {pre:.4f}')\n",
    "    print(f'Recall    : {rec:.4f}')\n",
    "    print(f'F1 Score  : {f1:.4f}')\n",
    "    return acc, pre, rec, f1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8d07e11-481c-4bdf-a841-004df2e679df",
   "metadata": {},
   "source": [
    "#### Evaluate GaussianNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "cf0a293a-c95e-4668-955b-ec2dc533b509",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gaussian Naive Bayes Results : \n",
      "\n",
      "Accuracy  : 0.8115\n",
      "Precision : 0.6933\n",
      "Recall    : 0.9547\n",
      "F1 Score  : 0.8033\n"
     ]
    }
   ],
   "source": [
    "print('Gaussian Naive Bayes Results : \\n')\n",
    "acc_gnb, pre_gnb, rec_gnb, f1_gnb = evaluate_model(X_test,y_test.values.flatten(),gnb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41818901-dc1a-40c6-9821-01abba17bd79",
   "metadata": {},
   "source": [
    "#### Evaluate BernoulliNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "c6ad7e14-0c44-4f8b-ab1a-c66607bdd474",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bernoulli Naive Bayes Results : \n",
      "\n",
      "Accuracy  : 0.8983\n",
      "Precision : 0.9044\n",
      "Recall    : 0.8362\n",
      "F1 Score  : 0.8690\n"
     ]
    }
   ],
   "source": [
    "print('Bernoulli Naive Bayes Results : \\n')\n",
    "acc_bnb, pre_bnb, rec_bnb, f1_bnb = evaluate_model(X_test,y_test.values.flatten(),ber)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87f734d9-312f-40b4-99ff-84cadc2ac614",
   "metadata": {},
   "source": [
    "#### Evaluate MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "872c6a5a-d5d9-4431-9eec-6dfab77962d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multinomial Naive Bayes Results : \n",
      "\n",
      "Accuracy  : 0.7897\n",
      "Precision : 0.7534\n",
      "Recall    : 0.7112\n",
      "F1 Score  : 0.7317\n"
     ]
    }
   ],
   "source": [
    "print('Multinomial Naive Bayes Results : \\n')\n",
    "acc_mnb, pre_mnb, rec_mnb, f1_mnb = evaluate_model(X_test,y_test.values.flatten(),mul)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed499756-d045-453b-b02c-c369536ac04e",
   "metadata": {},
   "source": [
    "### Discussion:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "137161ce-385a-4517-841b-e7abdb0688a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a dictionary for dataframe\n",
    "dct = {\n",
    "    'score':['accuracy','precision','recall','f1'],\n",
    "    'Gaussian':[acc_gnb,pre_gnb,rec_gnb,f1_gnb],\n",
    "    'Bernoulli':[acc_bnb,pre_bnb,rec_bnb,f1_bnb],\n",
    "    'Multinomial':[acc_mnb,pre_mnb,rec_mnb,f1_mnb]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "0cfb920c-cad6-4539-b2a8-84996523c50a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>score</th>\n",
       "      <th>Gaussian</th>\n",
       "      <th>Bernoulli</th>\n",
       "      <th>Multinomial</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>accuracy</td>\n",
       "      <td>0.811468</td>\n",
       "      <td>0.898349</td>\n",
       "      <td>0.789748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>precision</td>\n",
       "      <td>0.693271</td>\n",
       "      <td>0.904429</td>\n",
       "      <td>0.753425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>recall</td>\n",
       "      <td>0.954741</td>\n",
       "      <td>0.836207</td>\n",
       "      <td>0.711207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>f1</td>\n",
       "      <td>0.803264</td>\n",
       "      <td>0.868981</td>\n",
       "      <td>0.731707</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       score  Gaussian  Bernoulli  Multinomial\n",
       "0   accuracy  0.811468   0.898349     0.789748\n",
       "1  precision  0.693271   0.904429     0.753425\n",
       "2     recall  0.954741   0.836207     0.711207\n",
       "3         f1  0.803264   0.868981     0.731707"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Creating a DataFrame\n",
    "df_compare = pd.DataFrame(dct)\n",
    "df_compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "6646c526-ef13-4a67-b3e8-f59f343824ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "dct_crossval = {\n",
    "    'models':['Gaussian','Bernoulli','Multinomial'],\n",
    "    'cross_val_score_mean':[mean_score_gnb,mean_score_bnb,mean_score_mnb]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "9393e506-adc2-40fb-b3d7-8e60039f095b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>models</th>\n",
       "      <th>cross_val_score_mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Gaussian</td>\n",
       "      <td>0.827826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Bernoulli</td>\n",
       "      <td>0.847635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Multinomial</td>\n",
       "      <td>0.737251</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        models  cross_val_score_mean\n",
       "0     Gaussian              0.827826\n",
       "1    Bernoulli              0.847635\n",
       "2  Multinomial              0.737251"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_crossval = pd.DataFrame(dct_crossval)\n",
    "df_crossval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02785eb6-9f10-4f91-a203-e0bd434f6e49",
   "metadata": {},
   "source": [
    "### Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "630472db-3b0b-42a6-9df8-f356a6a9f522",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Summarise your findings and provide some suggestions for future work.\n",
    "\n",
    "Best Model for above data is Bernoulli Naive Bayes\n",
    "\n",
    "Bernoulli Naive Bayes is best model because of below reasons :\n",
    "\n",
    "BernoulliNB has highest test f1 score of 0.8509\n",
    "\n",
    "BernoulliNB has highest test accuracy of 0.8870\n",
    "\n",
    "BernoulliNB has highest 10 fold cross validation F1 score of 0.8492\n",
    "\n",
    "Although Naive Bayes algorithm is a powerful and widely used algorithm, it also has some limitations, including:\n",
    "\n",
    "1.The assumption of feature independence: The Naive Bayes algorithm assumes that the features are independent of each other. However, in real-world scenarios, this assumption is not always true, and features may be dependent on each other.\n",
    "\n",
    "2.Sensitivity to input data: Naive Bayes algorithm is very sensitive to input data, and even a slight change in the input data can significantly affect the accuracy of the model.\n",
    "\n",
    "3.Lack of tuning parameters: Naive Bayes algorithm does not have many tuning parameters that can be adjusted to improve its performance.\n",
    "\n",
    "4.Data sparsity problem: Naive Bayes algorithm relies on a lot of training data to estimate the probabilities of different features. However, if some features have very low frequencies in the training data, the algorithm may not be able to accurately estimate their probabilities.\n",
    "\n",
    "5.Class-conditional independence assumption: Naive Bayes algorithm assumes that each feature is conditionally independent given the class. However, in many cases, this assumption may not hold, and the algorithm may not perform well.\n",
    "\n",
    "6.Imbalanced class distribution: Naive Bayes algorithm assumes that the classes are equally likely, but in real-world scenarios, the class distribution may be imbalanced, which can lead to biased results.\n",
    "\n",
    "7.The need for continuous data: Naive Bayes algorithm assumes that the input features are continuous, which may not always be the case in real-world scenarios where the input features are discrete.\n",
    "\n",
    "Below are conclusions for above model\n",
    "\n",
    "1.Bernoulli Naive Bayes performed best on both cross validation and test dataset.\n",
    "\n",
    "2.For Email Classification Neural Network is better suited algorithm as it is able to provide better results and has lot of tunable paramenters.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa5dce18-84e8-4bae-8ad8-aaf2acc509da",
   "metadata": {},
   "source": [
    "### Model saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "ee8767f4-b880-4da3-94c8-788badb239f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the BernoulliNB file to pickle for future use\n",
    "import pickle\n",
    "with open('BernoulliModel.pkl','wb') as f:\n",
    "    pickle.dump(ber,file=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c09aaa7-abd2-4ab3-81b5-2a94497a41a7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
