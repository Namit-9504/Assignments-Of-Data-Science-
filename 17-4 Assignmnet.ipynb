{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "813b9753-5f2a-4780-8e1e-73bdbf139828",
   "metadata": {},
   "source": [
    "## Q1. What is Gradient Boosting Regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7987d78e-02b8-4775-aec5-b701ac94940e",
   "metadata": {},
   "source": [
    "Gradient Boosting Regression is a machine learning technique used for regression tasks. It is a powerful ensemble learning method that combines the predictions of multiple weak learners (typically decision trees) to create a strong regression model. Gradient Boosting Regression is an extension of the more general Gradient Boosting Machines (GBM) algorithm, which can be used for both regression and classification tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddf8f438-e0d8-44f5-ae65-c7a3491f5284",
   "metadata": {},
   "source": [
    "## Q2. Implement a simple gradient boosting algorithm from scratch using Python and NumPy. Use a simple regression problem as an example and train the model on a small dataset. Evaluate the model's performance using metrics such as mean squared error and R-squared."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9aaf4613-e1c9-43f3-96c6-6f5d15d44219",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean squared error: 822.8074571899081\n",
      "R-squared: 0.7836513313762161\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import make_regression\n",
    "X,Y = make_regression(n_samples=1000,n_features=1,n_informative=1, noise=20,random_state=43)\n",
    "# Split dataset into train and test sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "xtrain, xtest, ytrain, ytest = train_test_split(X,Y,test_size=0.2,random_state=42)\n",
    "# Define number of trees and learning rate\n",
    "n_estimators = 1000\n",
    "learning_rate = 0.001\n",
    "# Initialize ensemble predictions to the mean of the target variable\n",
    "import numpy as np\n",
    "ensemble_preds = np.full_like(ytrain, np.mean(ytrain))\n",
    "# Train the model using gradient boosting\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "stubs = []\n",
    "for i in range(n_estimators):\n",
    "    # Compute the residual between the current predictions and the true target values\n",
    "    residuals = ytrain - ensemble_preds\n",
    "    \n",
    "    # Fit a regression tree to the residuals\n",
    "    tree = DecisionTreeRegressor(max_depth=3)\n",
    "    tree.fit(xtrain, residuals)\n",
    "\n",
    "    stubs.append(tree)\n",
    "    \n",
    "    # Update the ensemble predictions with the current tree's predictions\n",
    "    ensemble_preds += learning_rate * tree.predict(xtrain)\n",
    "# Evaluate the model on the test set\n",
    "y_pred = np.full_like(ytest, np.mean(ytrain))\n",
    "for i in range(n_estimators):\n",
    "    y_pred += learning_rate * stubs[i].predict(xtest)\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "mse = mean_squared_error(ytest, y_pred)\n",
    "r2 = r2_score(ytest, y_pred)\n",
    "print(\"Mean squared error:\", mse)\n",
    "print(\"R-squared:\", r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da1062b8-eb32-4643-9f30-315aba55ff4b",
   "metadata": {},
   "source": [
    "## Q3. Experiment with different hyperparameters such as learning rate, number of trees, and tree depth to optimise the performance of the model. Use grid search or random search to find the best hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8f1a7d19-126c-4b5f-a322-8cd0a0f2c6ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters:  {'n_estimators': 100, 'max_depth': 5, 'learning_rate': 0.1}\n",
      "Best score:  -173.28617101455248\n",
      "Mean squared error:  164.57876327873456\n",
      "R-squared score:  0.9413966306233673\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Generate sample data\n",
    "X, y = make_regression(n_samples=1000, n_features=5, n_informative=3, noise=10, random_state=42)\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2,random_state=42)\n",
    "\n",
    "# Define the parameter grid for grid search\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'learning_rate': [0.01, 0.1, 0.5]\n",
    "}\n",
    "# Create a gradient boosting regressor object\n",
    "gbm = GradientBoostingRegressor()\n",
    "\n",
    "# Create a grid search object\n",
    "ran_search = RandomizedSearchCV(gbm,param_grid, \n",
    "                           scoring='neg_mean_squared_error',cv=5, n_jobs=-1)\n",
    "\n",
    "# Fit the grid search object to the data\n",
    "ran_search.fit(X_train, y_train)\n",
    "\n",
    "# Print the best hyperparameters and their corresponding score\n",
    "print(\"Best hyperparameters: \", ran_search.best_params_)\n",
    "print(\"Best score: \", ran_search.best_score_)\n",
    "\n",
    "# Evaluate the performance of the model on the test set\n",
    "y_pred = ran_search.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(\"Mean squared error: \", mse)\n",
    "print(\"R-squared score: \", r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31e92268-4b01-4d9e-a424-c32beb349163",
   "metadata": {},
   "source": [
    "## Q4. What is a weak learner in Gradient Boosting?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50fcdabb-84f1-4b11-90be-cdaec9988586",
   "metadata": {},
   "source": [
    "In Gradient Boosting, a weak learner refers to a base or individual learning algorithm that performs slightly better than random guessing but is not strong enough to make accurate predictions on its own. The concept of weak learners is fundamental to the Gradient Boosting ensemble learning technique.\n",
    "\n",
    "Gradient Boosting is an iterative ensemble method that aims to build a strong learner (a powerful predictive model) by combining the predictions of multiple weak learners. Each weak learner focuses on learning from the errors or residuals made by the previous weak learners in the ensemble. The process continues iteratively, with each new weak learner attempting to correct the mistakes of the previous ones.\n",
    "\n",
    "Typically, the weak learners used in Gradient Boosting are decision trees, often referred to as \"decision stumps\" when they are small trees with only a few splits. Decision trees are relatively simple and have limited depth, which makes them weak learners as they tend to have lower accuracy compared to more complex models.\n",
    "\n",
    "Despite their simplicity, when combined effectively in the Gradient Boosting process, these weak learners contribute to creating a powerful ensemble model capable of making accurate predictions and handling complex datasets. By focusing on the errors and updating the weights during each iteration, Gradient Boosting harnesses the collective strength of these weak learners to produce a strong, accurate predictive model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc5fc414-d9d7-4fa5-aa4a-9ba102971693",
   "metadata": {},
   "source": [
    "## Q5. What is the intuition behind the Gradient Boosting algorithm?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6adc28ff-d9ef-4ef6-905c-347914519bb2",
   "metadata": {},
   "source": [
    "The Gradient Boosting algorithm's intuition lies in the idea of building a strong predictive model by iteratively combining multiple weak learners (typically decision trees) to correct the errors made by the previous learners. The process can be summarized as follows:\n",
    "\n",
    "1. Initialization: The algorithm starts with an initial guess for the target variable (prediction) for all data points. This initial prediction can be a simple value, such as the mean of the target variable for regression problems or the mode for classification problems.\n",
    "\n",
    "2. Building Weak Learners: In each iteration, a weak learner (decision tree) is trained on the data to predict the errors or residuals between the current predictions and the actual target values. The weak learner's primary task is to identify the patterns in the data that the current model is struggling to capture accurately.\n",
    "\n",
    "3. Updating Predictions: The predictions from the weak learner are combined with the current predictions from the previous iteration. This combination is done by adding a fraction (learning rate) of the weak learner's predictions to the current predictions. The learning rate controls the step size of the updates and helps prevent overfitting.\n",
    "\n",
    "4. Iterative Refinement: The process is repeated for a predefined number of iterations (boosting rounds). In each iteration, the algorithm focuses on the errors made by the previous model and attempts to correct them. The weak learners are adjusted based on the gradients (derivatives) of the loss function with respect to the current predictions.\n",
    "\n",
    "5. Final Prediction: The final prediction is obtained by summing up the predictions from all the weak learners. The boosting process allows the model to learn complex relationships within the data by iteratively refining the predictions and minimizing the overall loss function.\n",
    "\n",
    "Intuitively, Gradient Boosting can be visualized as a team of weak learners collaborating to solve a complex problem. Each learner brings its unique perspective, focusing on the mistakes made by the previous learners and making small adjustments to improve overall accuracy. As the process continues, the collective knowledge of the team improves, resulting in a powerful ensemble model capable of making accurate predictions on the given dataset.\n",
    "\n",
    "The key advantages of Gradient Boosting include its ability to handle complex data, provide high predictive accuracy, and robustness against overfitting. However, it is essential to fine-tune the hyperparameters, such as the number of iterations and learning rate, to achieve the best performance for a particular problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd7f6351-650a-4af5-b5d4-ba2c75f398d2",
   "metadata": {},
   "source": [
    "## Q6. How does Gradient Boosting algorithm build an ensemble of weak learners?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89ac746b-b4c7-485e-a3d7-10a23499b38d",
   "metadata": {},
   "source": [
    "The Gradient Boosting algorithm builds an ensemble of weak learners in an iterative manner. Each weak learner is a decision tree, often referred to as a \"decision stump\" when it is a small tree with only a few splits. The process of building the ensemble can be summarized in the following steps:\n",
    "\n",
    "1. Initialization: The algorithm starts with an initial prediction for all data points. For regression problems, this initial prediction can be the mean of the target variable, while for classification problems, it can be the class with the highest frequency in the training data.\n",
    "\n",
    "2. Compute Residuals: In each iteration (boosting round), the algorithm calculates the residuals or errors between the current predictions and the actual target values. These residuals represent the information that the current model has failed to capture accurately.\n",
    "\n",
    "3. Train Weak Learner: A weak learner (decision tree) is trained on the training data to predict the residuals. The goal of the weak learner is to identify the patterns or relationships in the data that can help correct the errors made by the current model.\n",
    "\n",
    "4. Adjust Predictions: The predictions from the weak learner are combined with the current predictions from the previous iteration. The combination is done by adding a fraction (learning rate) of the weak learner's predictions to the current predictions. The learning rate controls the step size of the updates and helps prevent overfitting.\n",
    "\n",
    "5. Update Residuals: The residuals are updated using the predictions from the weak learner. This updated residual becomes the new target variable for the next iteration, and the process repeats.\n",
    "\n",
    "6. Repeat Iteratively: The process is repeated for a predefined number of iterations (boosting rounds). Each iteration focuses on the errors made by the previous model and attempts to correct them. The weak learners are adjusted based on the gradients (derivatives) of the loss function with respect to the current predictions.\n",
    "\n",
    "7. Final Ensemble: The final prediction is obtained by summing up the predictions from all the weak learners. The boosting process allows the model to learn complex relationships within the data by iteratively refining the predictions and minimizing the overall loss function.\n",
    "\n",
    "By iteratively building and adjusting the weak learners based on the errors of the previous models, Gradient Boosting creates a strong ensemble model that can effectively capture complex patterns in the data. The combination of multiple weak learners with different perspectives allows the model to generalize well and make accurate predictions on new, unseen data. This ensemble approach is one of the key strengths of the Gradient Boosting algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36e10696-f949-40f5-b543-6804d642f9af",
   "metadata": {},
   "source": [
    "## Q7. What are the steps involved in constructing the mathematical intuition of Gradient Boosting algorithm?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b3645bc-5117-47bc-b7a2-19c9e0a5f24f",
   "metadata": {},
   "source": [
    "Constructing the mathematical intuition of the Gradient Boosting algorithm involves understanding the underlying principles and equations used in the process. Here are the key steps involved in building the mathematical intuition of Gradient Boosting:\n",
    "\n",
    "1. Loss Function: The first step is to define a loss function that measures the difference between the model's predictions and the actual target values. For regression problems, a common loss function is the Mean Squared Error (MSE), while for classification problems, the Cross-Entropy (Log Loss) is often used. The loss function quantifies the model's performance, and the goal is to minimize it during the learning process.\n",
    "\n",
    "2. Initial Prediction: The algorithm starts with an initial prediction for all data points. In the case of regression, the initial prediction can be the mean of the target variable, while for classification, it can be the class with the highest frequency in the training data.\n",
    "\n",
    "3. Residual Calculation: In each iteration (boosting round), the residuals are computed by taking the difference between the actual target values and the current predictions. These residuals represent the information that the current model has failed to capture accurately.\n",
    "\n",
    "4. Weak Learner (Decision Tree): The weak learner (base learner) used in Gradient Boosting is typically a decision tree. Decision trees are simple models that can handle both numerical and categorical data, making them suitable for various types of problems. A weak learner is trained on the training data to predict the residuals.\n",
    "\n",
    "5. Learning Rate: The learning rate is a hyperparameter that controls the step size of the updates in each iteration. It is multiplied by the predictions from the weak learner before being added to the current predictions. A smaller learning rate helps prevent overfitting by making smaller adjustments to the model in each step.\n",
    "\n",
    "6. Ensemble Building: The predictions from the weak learner are combined with the current predictions from the previous iteration. This combination is done by adding a fraction (learning rate * weak learner's predictions) to the current predictions. This process effectively corrects the errors made by the previous model.\n",
    "\n",
    "7. Update Residuals: The residuals are updated using the predictions from the weak learner. This updated residual becomes the new target variable for the next iteration. The process repeats iteratively, and in each iteration, the algorithm focuses on the errors made by the previous model and attempts to correct them.\n",
    "\n",
    "8. Final Prediction: The final prediction is obtained by summing up the predictions from all the weak learners. The boosting process allows the model to learn complex relationships within the data by iteratively refining the predictions and minimizing the overall loss function.\n",
    "\n",
    "9. Regularization: To prevent overfitting, Gradient Boosting often employs regularization techniques such as limiting the depth of the decision trees (tree pruning) and introducing randomization (stochastic gradient boosting).\n",
    "\n",
    "By understanding these steps and the mathematical equations involved, one can gain a deeper insight into how the Gradient Boosting algorithm effectively combines weak learners to build a strong ensemble model capable of making accurate predictions on various types of problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6699e1ed-5cc1-44e7-adee-90e5486314ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b1ae512-484e-4c9b-9369-3a323714add3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a4f2e0b-bb65-4abd-85b1-7b42de3306f1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
