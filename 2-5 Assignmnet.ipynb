{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a594f883-6189-4a72-b173-11d280fc56cb",
   "metadata": {},
   "source": [
    "## Q1. What is anomaly detection and what is its purpose?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38764380-3c3b-421a-8519-5e734358e51a",
   "metadata": {},
   "source": [
    "Anomaly detection, also known as outlier detection, is a machine learning technique used to identify rare and unusual patterns or observations in data that significantly differ from the majority of the data points. These rare patterns are called anomalies or outliers.\n",
    "\n",
    "The purpose of anomaly detection is to identify instances that deviate from the typical behavior or expected patterns in a dataset. Anomalies can represent critical events, errors, fraud, faults, or other exceptional circumstances that are different from the norm. By detecting anomalies, we can gain valuable insights into unusual occurrences, potential issues, or hidden opportunities.\n",
    "\n",
    "Anomaly detection is applied across various domains and industries, including:\n",
    "\n",
    "1. Intrusion Detection: Detecting unusual network activities that might indicate cyber-attacks or security breaches.\n",
    "\n",
    "2. Fraud Detection: Identifying suspicious transactions or activities that may indicate fraudulent behavior.\n",
    "\n",
    "3. Health Monitoring: Detecting unusual health conditions or abnormal symptoms in medical data.\n",
    "\n",
    "4. Predictive Maintenance: Identifying anomalies in machinery or equipment sensor data to predict maintenance needs and prevent failures.\n",
    "\n",
    "5. Quality Control: Detecting defects or irregularities in manufacturing processes.\n",
    "\n",
    "6. Financial Analysis: Identifying unusual patterns in financial transactions or stock market data.\n",
    "\n",
    "7. Environmental Monitoring: Detecting abnormal events in environmental sensor data.\n",
    "\n",
    "Anomaly detection methods can be supervised, semi-supervised, or unsupervised, depending on the availability of labeled data. Unsupervised anomaly detection is the most common approach since it does not require labeled examples of anomalies during training.\n",
    "\n",
    "Various algorithms and techniques can be used for anomaly detection, such as statistical methods, clustering, density-based approaches, and machine learning models like isolation forests, one-class SVM, autoencoders, and local outlier factor (LOF).\n",
    "\n",
    "Overall, anomaly detection is a crucial tool for identifying unusual events or data points that might otherwise go unnoticed, enabling organizations to take appropriate actions, improve system reliability, and ensure the integrity of their data and processes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61315815-98f6-46ab-9ef7-46bab94ffedf",
   "metadata": {},
   "source": [
    "## Q2. What are the key challenges in anomaly detection?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48513fb3-41ed-4ff3-81b1-67f40a1025af",
   "metadata": {},
   "source": [
    "Anomaly detection comes with its set of challenges, which can make the task more complex and require careful consideration when implementing anomaly detection systems. Some of the key challenges in anomaly detection include:\n",
    "\n",
    "1. Lack of Labeled Anomaly Data: In many real-world applications, obtaining labeled anomaly data can be difficult and expensive. Anomaly detection methods typically fall into unsupervised or semi-supervised approaches to address this challenge. However, unsupervised methods might struggle to distinguish between normal and novel anomalies without labeled examples.\n",
    "\n",
    "2. Imbalanced Data: Anomaly detection often deals with imbalanced datasets where the number of normal instances significantly outweighs the number of anomalies. This imbalance can lead to biased models that are better at detecting the majority class (normal instances) but perform poorly on detecting anomalies.\n",
    "\n",
    "3. Choosing the Right Anomaly Detection Method: There is no one-size-fits-all approach to anomaly detection, and the choice of the appropriate method depends on the nature of the data and the specific problem at hand. Selecting the right technique that suits the data distribution and anomaly types can be challenging.\n",
    "\n",
    "4. Seasonality and Temporal Dependencies: In time-series data, anomalies can exhibit seasonal patterns or temporal dependencies. Detecting anomalies in such data requires specialized techniques that can account for the temporal aspects and distinguish between expected fluctuations and real anomalies.\n",
    "\n",
    "5. Data Dimensionality: High-dimensional data can present challenges for traditional anomaly detection methods. As the dimensionality increases, the density of data points may become sparse, making it harder to distinguish normal and anomalous instances. Dimensionality reduction techniques can be used to alleviate this problem.\n",
    "\n",
    "6. Novelty Detection: Anomaly detection systems often encounter novel or previously unseen anomalies. Detecting such novel anomalies can be a challenging task, especially when the system was trained on a specific set of anomalies.\n",
    "\n",
    "7. Interpretability: In some applications, it is essential to understand why a particular instance is flagged as an anomaly. However, certain anomaly detection methods, especially deep learning-based approaches, can be less interpretable, making it harder to explain the decision-making process.\n",
    "\n",
    "8. Real-Time Detection: Real-time anomaly detection requires quick and efficient processing of data. Computationally intensive algorithms may not be suitable for applications that demand immediate response times.\n",
    "\n",
    "9. Concept Drift: Anomaly detection models may need to adapt to changes in the data distribution over time, known as concept drift. A model that worked well initially might become less effective if the data patterns shift significantly.\n",
    "\n",
    "10. False Positives and False Negatives: Striking a balance between minimizing false positives (normal instances incorrectly classified as anomalies) and false negatives (anomalies missed by the system) is essential. Reducing false positives might lead to more false negatives and vice versa.\n",
    "\n",
    "Addressing these challenges often involves a combination of domain knowledge, data preprocessing, algorithm selection, and model evaluation techniques to build effective and reliable anomaly detection systems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90495807-dff6-4823-b560-18a5b45d98ec",
   "metadata": {},
   "source": [
    "## Q3. How does unsupervised anomaly detection differ from supervised anomaly detection?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3f80f73-f21d-4ae5-9b08-7fa12d94e025",
   "metadata": {},
   "source": [
    "Unsupervised anomaly detection and supervised anomaly detection are two distinct approaches to identifying anomalies in data, and they differ in their underlying methodologies and requirements:\n",
    "\n",
    "1. **Training Data:**\n",
    "   - **Unsupervised Anomaly Detection:** In unsupervised anomaly detection, the algorithm is trained on a dataset that contains only normal (non-anomalous) instances. The model learns the patterns and structures inherent in the normal data without explicit knowledge of anomalies.\n",
    "   - **Supervised Anomaly Detection:** In supervised anomaly detection, the algorithm is trained on a dataset that contains both normal instances and labeled anomalous instances. The model learns the patterns of both normal and anomalous data and is explicitly provided with labeled examples of anomalies during training.\n",
    "\n",
    "2. **Labeling Anomalies:**\n",
    "   - **Unsupervised Anomaly Detection:** Unsupervised methods do not require labeled anomaly data during training. The model detects anomalies based on deviations from the normal data distribution, without knowing the specific class labels of anomalies.\n",
    "   - **Supervised Anomaly Detection:** Supervised methods rely on labeled anomalies during training, and the model learns to distinguish between normal and anomalous instances based on these labels.\n",
    "\n",
    "3. **Model Complexity:**\n",
    "   - **Unsupervised Anomaly Detection:** Unsupervised methods often have simpler model architectures since they only need to represent the normal data distribution and identify deviations from it.\n",
    "   - **Supervised Anomaly Detection:** Supervised methods may require more complex models to learn the patterns of both normal and anomalous data, especially if anomalies are highly diverse or if the feature space is complex.\n",
    "\n",
    "4. **Application and Detection Capability:**\n",
    "   - **Unsupervised Anomaly Detection:** Unsupervised methods are more suitable when anomalies are rare and unknown in the dataset. They are useful for outlier detection and can identify novel anomalies that were not present in the training data.\n",
    "   - **Supervised Anomaly Detection:** Supervised methods are effective when anomalies are well-defined and labeled in the training data. They can provide a more precise detection of known anomalies based on their labeled characteristics.\n",
    "\n",
    "5. **Data Availability:**\n",
    "   - **Unsupervised Anomaly Detection:** Unsupervised methods are more versatile as they can be applied to datasets where labeled anomaly data is not available or challenging to obtain.\n",
    "   - **Supervised Anomaly Detection:** Supervised methods require labeled anomaly data, which may be a limitation in certain scenarios.\n",
    "\n",
    "6. **Performance Evaluation:**\n",
    "   - **Unsupervised Anomaly Detection:** Evaluating the performance of unsupervised methods is more challenging since there are no labeled anomalies to directly compare with the results. Evaluation often relies on metrics like the reconstruction error or density estimation.\n",
    "   - **Supervised Anomaly Detection:** Supervised methods can be evaluated using standard supervised learning metrics like accuracy, precision, recall, and F1-score, as they have access to labeled anomalies for comparison.\n",
    "\n",
    "Both approaches have their strengths and weaknesses, and the choice between unsupervised and supervised anomaly detection depends on the availability of labeled data, the nature of anomalies, and the specific requirements of the problem at hand. In many real-world scenarios, unsupervised anomaly detection is preferred due to its ability to handle novel anomalies and its flexibility in dealing with data without labeled anomalies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b056df9a-31e2-4200-a501-5b65b8cf8a73",
   "metadata": {},
   "source": [
    "## Q4. What are the main categories of anomaly detection algorithms?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bb286e5-25c5-487b-b200-e7f28b27b86e",
   "metadata": {},
   "source": [
    "Anomaly detection algorithms can be broadly categorized into the following main categories based on their underlying methodologies and approaches:\n",
    "\n",
    "1. **Statistical Methods:**\n",
    "   - Statistical methods assume that normal data follows a known statistical distribution, such as Gaussian (Normal) distribution. Anomalies are then identified as data points that deviate significantly from this expected distribution.\n",
    "   - Common statistical techniques for anomaly detection include Z-score, Grubbs' test, Dixon's Q-test, and the use of probability density functions.\n",
    "\n",
    "2. **Distance-Based Methods:**\n",
    "   - Distance-based methods identify anomalies by measuring the distance or dissimilarity between data points. Anomalies are often points that are far away from the majority of the data points in the feature space.\n",
    "   - The k-Nearest Neighbors (KNN) algorithm is an example of a distance-based method commonly used for anomaly detection.\n",
    "\n",
    "3. **Density-Based Methods:**\n",
    "   - Density-based methods focus on estimating the density of the data in the feature space and identify anomalies as data points that fall in low-density regions, where the data is sparse.\n",
    "   - One popular density-based algorithm is the Local Outlier Factor (LOF) algorithm.\n",
    "\n",
    "4. **Clustering-Based Methods:**\n",
    "   - Clustering-based methods group similar data points into clusters and identify anomalies as data points that do not belong to any cluster or form small, isolated clusters.\n",
    "   - One example of a clustering-based approach for anomaly detection is the DBSCAN (Density-Based Spatial Clustering of Applications with Noise) algorithm.\n",
    "\n",
    "5. **Machine Learning-Based Methods:**\n",
    "   - Machine learning-based methods leverage supervised or unsupervised learning techniques to model the normal data distribution and identify deviations from it.\n",
    "   - Unsupervised learning methods like Autoencoders, Isolation Forest, and One-Class Support Vector Machines (SVM) are often used for unsupervised anomaly detection.\n",
    "   - Supervised learning methods can be used in supervised anomaly detection when labeled anomaly data is available.\n",
    "\n",
    "6. **Ensemble Methods:**\n",
    "   - Ensemble methods combine multiple anomaly detection algorithms or models to improve overall performance and robustness. The idea is that different algorithms may capture different aspects of anomalies, and combining them can lead to better results.\n",
    "   - Examples of ensemble techniques include bagging, boosting, and model stacking.\n",
    "\n",
    "7. **Deep Learning-Based Methods:**\n",
    "   - Deep learning methods, particularly deep neural networks, have been increasingly used for anomaly detection tasks, especially when dealing with high-dimensional and complex data.\n",
    "   - Variational Autoencoders (VAEs), Generative Adversarial Networks (GANs), and Long Short-Term Memory (LSTM) networks are commonly applied in deep learning-based anomaly detection.\n",
    "\n",
    "Each category of anomaly detection algorithms has its strengths and weaknesses, and the choice of the most appropriate method depends on the specific characteristics of the data, the nature of anomalies, and the available resources. In practice, a combination of different algorithms or an ensemble approach is often used to achieve better anomaly detection performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "380882e7-d334-4b3e-910d-c9b506b92c6c",
   "metadata": {},
   "source": [
    "## Q5. What are the main assumptions made by distance-based anomaly detection methods?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d5a68db-b139-4fc0-8000-de7cd7e6db0d",
   "metadata": {},
   "source": [
    "Distance-based anomaly detection methods make several key assumptions about the underlying data distribution and the nature of anomalies. These assumptions are important for the methods to effectively identify anomalies based on the concept of distance or dissimilarity between data points. The main assumptions made by distance-based anomaly detection methods include:\n",
    "\n",
    "1. **Distance Measure:** Distance-based methods assume the availability of a meaningful distance or dissimilarity measure to quantify the similarity between data points in the feature space. Common distance metrics include Euclidean distance, Manhattan distance, and cosine similarity.\n",
    "\n",
    "2. **Normal Data Distribution:** Distance-based methods assume that the majority of the data points follow a specific data distribution or cluster tightly around the center of the distribution. This center represents the typical or normal behavior of the data.\n",
    "\n",
    "3. **Global Structure:** Distance-based methods often assume that the majority of normal data points exhibit a relatively consistent global structure in the feature space. Anomalies are expected to deviate significantly from this global structure.\n",
    "\n",
    "4. **Anomalies as Outliers:** Distance-based methods view anomalies as outliers that are located far away from the bulk of normal data points. Anomalies are considered to be isolated instances that do not follow the normal pattern exhibited by the majority of the data.\n",
    "\n",
    "5. **Density Variation:** Some distance-based methods take into account the variation in data density, assuming that anomalies are likely to be found in regions of low data density or sparse regions of the feature space.\n",
    "\n",
    "6. **Homogeneous Density:** Certain distance-based methods assume that the density of normal data is relatively homogeneous throughout the feature space, which means that the density does not vary significantly across different regions.\n",
    "\n",
    "7. **Single Density Cluster:** Some distance-based methods assume that normal data points belong to a single dense cluster, and anomalies are considered as data points outside this dense cluster.\n",
    "\n",
    "It is essential to be mindful of these assumptions when applying distance-based anomaly detection methods to real-world data. While these methods can be effective in certain scenarios, they may not perform optimally if the data does not adhere to these assumptions. Additionally, distance-based methods might have limitations in dealing with high-dimensional and sparse data or when anomalies form complex patterns that cannot be captured solely based on distance measures. Careful data analysis and understanding of the underlying data distribution are crucial for successful anomaly detection using distance-based methods."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80655e9f-50d0-49b8-9d36-b6f542cdf53d",
   "metadata": {},
   "source": [
    "## Q6. How does the LOF algorithm compute anomaly scores?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "976a06b2-81a4-4067-a85e-ccf61c1052c8",
   "metadata": {},
   "source": [
    "The LOF (Local Outlier Factor) algorithm computes anomaly scores for each data point in a dataset by measuring its local density in relation to its neighbors. The LOF algorithm is a density-based anomaly detection method, and it identifies anomalies based on how isolated a data point is compared to its local neighborhood. Higher LOF scores indicate that a data point is more likely to be an anomaly.\n",
    "\n",
    "Here are the steps involved in computing the LOF anomaly scores for a data point:\n",
    "\n",
    "1. **Determine the Neighborhood:** For each data point in the dataset, the algorithm identifies its k-nearest neighbors based on a distance metric (e.g., Euclidean distance). The parameter k is a user-defined value representing the number of neighbors considered.\n",
    "\n",
    "2. **Compute Reachability Distance:** For each data point and its k-nearest neighbors, the reachability distance is calculated. The reachability distance of a data point A from its neighbor B is the maximum of the distance between A and B or the k-distance of B. The k-distance of a point is the distance to its k-th nearest neighbor.\n",
    "\n",
    "3. **Calculate Local Reachability Density (LRD):** The local reachability density of a data point A is computed by taking the reciprocal of the average reachability distance between A and its k-nearest neighbors. A higher LRD indicates that the data point is surrounded by neighbors in a relatively dense region.\n",
    "\n",
    "4. **Calculate Local Outlier Factor (LOF):** The LOF of a data point A is the ratio of the average LRD of its k-nearest neighbors to its own LRD. The LOF quantifies how much the density of A differs from the densities of its neighbors. If the LOF value is close to 1, it means that the density of A is similar to its neighbors, and it is considered a normal data point. However, if the LOF value is significantly greater than 1, it indicates that A is less dense than its neighbors, making it a potential outlier.\n",
    "\n",
    "5. **Final Anomaly Score:** The LOF algorithm assigns an anomaly score to each data point, representing the extent to which the point deviates from its local neighborhood's density. Higher LOF scores suggest that a data point is an outlier or anomaly.\n",
    "\n",
    "The LOF algorithm identifies anomalies based on the relative density of data points, allowing it to detect outliers in complex and non-linear data distributions. It does not rely on a global density threshold, making it more robust to variations in data density across different regions of the feature space."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d49350c4-981a-45df-b71a-cc70c99ee85c",
   "metadata": {},
   "source": [
    "## Q7. What are the key parameters of the Isolation Forest algorithm?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86080bbd-863d-45ca-940e-2a7e0c1f0408",
   "metadata": {},
   "source": [
    "\n",
    "The Isolation Forest algorithm is an unsupervised machine learning algorithm used for anomaly detection. It is based on the concept of isolating anomalies in data by recursively partitioning the feature space. The algorithm creates random partitions, and anomalies are expected to be easier to isolate and require fewer partitions compared to normal data points. The key parameters of the Isolation Forest algorithm include:\n",
    "\n",
    "1. **n_estimators (or n_trees):**\n",
    "   - This parameter represents the number of isolation trees to be built in the forest. Increasing the number of trees can improve the performance of the algorithm, but it also increases computation time.\n",
    "\n",
    "2. **max_samples:**\n",
    "   - It determines the maximum number of samples used to build each isolation tree. By default, it is set to \"auto,\" which means it uses min(256, n_samples), where n_samples is the number of data points in the dataset. Setting max_samples to a smaller value can lead to a faster training process.\n",
    "\n",
    "3. **max_features:**\n",
    "   - This parameter determines the number of features to consider when performing splits in each isolation tree. By default, it is set to \"auto,\" which means it uses all features. You can set it to an integer value or a fraction to specify the number of features to use.\n",
    "\n",
    "4. **contamination:**\n",
    "   - This parameter specifies the proportion of anomalies in the data. It is used to set the threshold for identifying anomalies based on the isolation forest's output scores. The default value is \"auto,\" which automatically sets the contamination based on the assumption that anomalies are rare. You can also set it to a specific value if you have prior knowledge of the dataset.\n",
    "\n",
    "5. **bootstrap:**\n",
    "   - This parameter determines whether to use bootstrapping when sampling data to build each isolation tree. Bootstrapping means that each tree is built on a random sample with replacement from the original dataset. By default, it is set to True.\n",
    "\n",
    "6. **random_state:**\n",
    "   - It is used to control the random number generator's seed for reproducibility. Setting a specific random_state ensures that the algorithm produces the same results each time it is run with the same parameters and data.\n",
    "\n",
    "These parameters allow you to customize the behavior of the Isolation Forest algorithm based on the characteristics of your data and the specific anomaly detection task. Tuning these parameters can help improve the performance of the algorithm and its ability to detect anomalies effectively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "430bbfbc-9ff0-447c-9beb-e99994ce77bc",
   "metadata": {},
   "source": [
    "## Q8. If a data point has only 2 neighbours of the same class within a radius of 0.5, what is its anomaly score using KNN with K=10?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3f30a7b-22d2-4a7a-a14b-0765acee705d",
   "metadata": {},
   "source": [
    "To calculate the anomaly score of a data point using KNN with K=10, we need to compute its Local Outlier Factor (LOF) value based on its local neighborhood. Given that the data point has only 2 neighbors of the same class within a radius of 0.5, we can proceed with the steps of the LOF algorithm:\n",
    "\n",
    "Step 1: Determine the Neighborhood (k-nearest neighbors)\n",
    "Since K=10, we need to find the 10 nearest neighbors of the data point.\n",
    "\n",
    "Step 2: Compute Reachability Distance\n",
    "We calculate the reachability distance between the data point and its 10 nearest neighbors. However, in this case, we have only 2 neighbors within a radius of 0.5. So, the other 8 neighbors fall outside this radius and are not considered.\n",
    "\n",
    "Step 3: Calculate Local Reachability Density (LRD)\n",
    "The LRD is calculated as the reciprocal of the average reachability distance between the data point and its k-nearest neighbors. However, since we have only 2 neighbors, we take the reciprocal of the average reachability distance between the data point and its 2 neighbors.\n",
    "\n",
    "Step 4: Calculate Local Outlier Factor (LOF)\n",
    "The LOF is computed as the ratio of the average LRD of the data point's k-nearest neighbors to its own LRD. Again, since we have only 2 neighbors, we take the ratio of the average LRD of these 2 neighbors to the LRD of the data point.\n",
    "\n",
    "Since the data point has only 2 neighbors of the same class within a radius of 0.5, it means that it is well within the dense region of its class. This results in a small LRD value for the data point. Consequently, the LOF value will be close to 1, indicating that the data point is similar in density to its neighbors and is not an outlier.\n",
    "\n",
    "Therefore, the anomaly score of the data point using KNN with K=10 in this case would be close to 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9ff5211-d9c8-4344-b493-8d7c30e1f753",
   "metadata": {},
   "source": [
    "## Q9. Using the Isolation Forest algorithm with 100 trees and a dataset of 3000 data points, what is the anomaly score for a data point that has an average path length of 5.0 compared to the average path length of the trees?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c275e825-0392-4446-8c1d-97aa288e064c",
   "metadata": {},
   "source": [
    "In the Isolation Forest algorithm, the anomaly score for a data point is computed based on its average path length in the isolation trees compared to the average path length of the trees. The average path length measures how deep the data point reaches into the trees during the isolation process.\n",
    "\n",
    "The anomaly score (s) for a data point with an average path length (l) is calculated as follows:\n",
    "\n",
    "s = 2^(-l / c)\n",
    "\n",
    "where 'c' is the average path length of the trees.\n",
    "\n",
    "In this case, the data point has an average path length of 5.0, and we are given that there are 100 trees in the isolation forest. However, we need the value of 'c' (the average path length of the trees) to calculate the anomaly score.\n",
    "\n",
    "The value of 'c' is usually not provided explicitly in the Isolation Forest algorithm, but it can be approximated as follows:\n",
    "\n",
    "c ≈ 2 * (log(n) - 1) / n\n",
    "\n",
    "where 'n' is the number of data points in the dataset (3000 in this case).\n",
    "\n",
    "Let's calculate the value of 'c':\n",
    "\n",
    "c ≈ 2 * (log(3000) - 1) / 3000\n",
    "c ≈ 2 * (8.006 - 1) / 3000\n",
    "c ≈ 14.012 / 3000\n",
    "c ≈ 0.0047\n",
    "\n",
    "Now, we can calculate the anomaly score for the data point with an average path length of 5.0:\n",
    "\n",
    "s = 2^(-5.0 / 0.0047)\n",
    "s ≈ 2^(-1063.83)\n",
    "s ≈ 1.013e-321\n",
    "\n",
    "The anomaly score for the data point is approximately 1.013e-321. A lower anomaly score indicates a higher likelihood of the data point being an anomaly or outlier in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99a41f01-7ebf-44f6-b7b4-5440d8db7805",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba2bed51-ee61-4d96-8d9b-c70e97e78302",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7985464f-1ab6-455f-84a4-4da2b1b48bb1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fa48d27-2d90-4ef4-b6b4-c08708541088",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c73f40c1-2f7a-41d4-a2df-10e5c0f5622b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
