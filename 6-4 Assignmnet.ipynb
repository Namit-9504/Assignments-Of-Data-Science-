{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e2eced83-e399-4a2e-9623-41da86766a8a",
   "metadata": {},
   "source": [
    "## Q1. What is the mathematical formula for a linear SVM?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed86a2af-37c8-4a21-89f4-392598c21637",
   "metadata": {},
   "source": [
    "\n",
    " Linear Support Vector Machine (SVM)\n",
    "\n",
    "The mathematical formula for a linear Support Vector Machine (SVM) can be represented as follows:\n",
    "\n",
    "Given a training dataset of labeled samples:\n",
    "\n",
    "\\( D = \\{(x_1, y_1), (x_2, y_2), ..., (x_n, y_n)\\} \\)\n",
    "\n",
    "where \\( x_i \\) represents the input features of sample \\( i \\) and \\( y_i \\) represents the corresponding class label (either +1 or -1 for a binary classification problem), the objective of the linear SVM is to find a hyperplane that best separates the two classes in the feature space.\n",
    "\n",
    "The hyperplane is represented by the equation:\n",
    "\n",
    "\\[ w \\cdot x + b = 0 \\]\n",
    "\n",
    "where \\( w \\) is the weight vector perpendicular to the hyperplane and \\( b \\) is the bias term that shifts the hyperplane parallel to itself.\n",
    "\n",
    "In a binary classification setting, the SVM aims to find the optimal hyperplane that maximizes the margin between the two classes. The margin is defined as the distance between the hyperplane and the closest samples from each class. The samples that lie on the margin are called support vectors, and they are crucial for determining the hyperplane.\n",
    "\n",
    "To ensure the margin is maximized and the hyperplane separates the classes correctly, the SVM optimization problem is typically formulated as follows:\n",
    "\n",
    "Minimize: \\( \\frac{1}{2} \\lVert w \\rVert^2 \\)\n",
    "\n",
    "subject to: \\( y_i (w \\cdot x_i + b) \\geq 1 \\) for all training samples \\( (x_i, y_i) \\)\n",
    "\n",
    "The inequality constraint ensures that the samples are classified correctly, i.e., they are on the correct side of the hyperplane, and the margin between the two classes is at least 1. The \\( \\frac{1}{2} \\lVert w \\rVert^2 \\) term is the regularization term, which helps to maximize the margin and avoid overfitting.\n",
    "\n",
    "The optimization problem is typically solved using techniques from convex optimization, such as the quadratic programming method. Once the optimal values of \\( w \\) and \\( b \\) are obtained, the hyperplane equation is defined, and the SVM can be used for classification of new, unseen data points.\n",
    "\n",
    "Please note that this equation is specific to the linear SVM, which deals with linearly separable data. For non-linearly separable data, kernels are used to map the input space into a higher-dimensional feature space, allowing the SVM to find a separating hyperplane."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "195b4f61-90e9-4e73-b13d-024938351419",
   "metadata": {},
   "source": [
    "## Q2. What is the objective function of a linear SVM?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dff7f44-1d15-404f-b7ee-67a2c04e95c8",
   "metadata": {},
   "source": [
    "The objective function of a linear Support Vector Machine (SVM) is to find the optimal hyperplane that maximizes the margin between the two classes in the feature space. The margin is defined as the distance between the hyperplane and the closest samples (support vectors) from each class.\n",
    "\n",
    "For a binary classification problem, where the class labels are either +1 or -1, the objective function of a linear SVM can be formulated as follows:\n",
    "\n",
    "Minimize: \\( \\frac{1}{2} \\lVert w \\rVert^2 \\)\n",
    "\n",
    "subject to: \\( y_i (w \\cdot x_i + b) \\geq 1 \\) for all training samples \\( (x_i, y_i) \\)\n",
    "\n",
    "Where:\n",
    "- \\( w \\) is the weight vector perpendicular to the hyperplane,\n",
    "- \\( b \\) is the bias term that shifts the hyperplane parallel to itself,\n",
    "- \\( x_i \\) represents the input features of sample \\( i \\),\n",
    "- \\( y_i \\) represents the corresponding class label (+1 or -1) for sample \\( i \\),\n",
    "- \\( \\lVert w \\rVert \\) denotes the Euclidean norm (magnitude) of the weight vector.\n",
    "\n",
    "The objective function consists of two parts: the regularization term \\( \\frac{1}{2} \\lVert w \\rVert^2 \\) and the inequality constraint \\( y_i (w \\cdot x_i + b) \\geq 1 \\). The regularization term helps to maximize the margin and avoid overfitting, while the inequality constraint ensures that all training samples are classified correctly and lie on the correct side of the hyperplane.\n",
    "\n",
    "In this formulation, the SVM aims to find the values of \\( w \\) and \\( b \\) that minimize \\( \\frac{1}{2} \\lVert w \\rVert^2 \\) while satisfying the inequality constraint for all training samples. The optimal values of \\( w \\) and \\( b \\) define the hyperplane equation, which can be used for classification of new, unseen data points.\n",
    "\n",
    "The optimization problem is typically solved using techniques from convex optimization, such as the quadratic programming method, to find the optimal values of \\( w \\) and \\( b \\) that fulfill the objective function and constraints. Once the optimization is complete, the SVM can be used as a linear classifier for the given binary classification task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2305951f-1b5e-441f-b424-6e4e2b4da4cb",
   "metadata": {},
   "source": [
    "## Q3. What is the kernel trick in SVM?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bccc236e-fa3c-498f-930e-9342daa1ef49",
   "metadata": {},
   "source": [
    "The kernel trick is a fundamental concept in Support Vector Machines (SVM) that allows SVMs to efficiently handle non-linearly separable data by implicitly mapping the input features into a higher-dimensional feature space. It enables linear classifiers, like SVMs, to effectively learn and classify non-linear patterns in the data.\n",
    "\n",
    "The basic idea behind the kernel trick is to avoid the explicit computation of the high-dimensional feature space, which may be computationally expensive or even impossible in some cases. Instead, the kernel function computes the dot product between the feature vectors in the high-dimensional space without explicitly transforming the data into that space. This is achieved by defining a kernel function \\( K(x, x') \\) that corresponds to the dot product of the feature vectors \\( \\phi(x) \\) and \\( \\phi(x') \\) in the high-dimensional space:\n",
    "\n",
    "\\[ K(x, x') = \\phi(x) \\cdot \\phi(x') \\]\n",
    "\n",
    "The kernel function allows us to implicitly operate in the higher-dimensional space while remaining in the original input space. This can be computationally advantageous, especially when the dimensionality of the higher-dimensional feature space is very large or even infinite.\n",
    "\n",
    "The kernel trick is most commonly used with SVMs to find a non-linear separating hyperplane in the transformed feature space. When using a kernel, the optimization problem of the SVM changes from finding a linear hyperplane in the original feature space to finding a linear hyperplane in the high-dimensional feature space. This enables the SVM to discover more complex decision boundaries and handle data that is not linearly separable in the original space.\n",
    "\n",
    "Some common kernel functions used in SVMs include:\n",
    "1. Linear kernel: \\( K(x, x') = x \\cdot x' \\) (same as the original dot product in the input space).\n",
    "2. Polynomial kernel: \\( K(x, x') = (x \\cdot x' + c)^d \\), where \\( c \\) and \\( d \\) are user-defined constants.\n",
    "3. Radial Basis Function (RBF) kernel: \\( K(x, x') = \\exp(-\\gamma \\lVert x - x' \\rVert^2) \\), where \\( \\gamma \\) is a user-defined parameter.\n",
    "\n",
    "By choosing an appropriate kernel function and its associated parameters, SVMs can efficiently handle non-linear data and achieve high accuracy in various classification tasks. The kernel trick is one of the key reasons why SVMs are widely used and highly effective in practice."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1041052f-9968-4a52-b52e-516242f0aa54",
   "metadata": {},
   "source": [
    "## Q4. What is the role of support vectors in SVM Explain with example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf046c72-0132-496d-a78a-0202732a1224",
   "metadata": {},
   "source": [
    "In Support Vector Machines (SVM), support vectors play a crucial role in defining the optimal hyperplane that separates the classes in a binary classification problem. Support vectors are the data points that lie closest to the separating hyperplane and have the most significant influence on its position and orientation. These are the points that \"support\" the definition of the hyperplane, and hence the name \"support vectors.\"\n",
    "\n",
    "To better understand the role of support vectors, let's consider a simple example of a 2-dimensional binary classification problem. Assume we have two classes, represented by red and blue points in the following scatter plot:\n",
    "\n",
    "```\n",
    "  |         *\n",
    "  |       *   *\n",
    "  |    *    *   *\n",
    "  |   *      *    *\n",
    "  | *       SV    *\n",
    "  |____________________\n",
    "         Feature 1\n",
    "```\n",
    "\n",
    "In this example, the circles (*) represent the data points from each class, and the \"SV\" represents the support vector from each class. We can observe the following:\n",
    "\n",
    "1. The support vectors are the points closest to the separating hyperplane (the line in 2D). They are the ones that have the smallest margin to the hyperplane.\n",
    "\n",
    "2. The other data points that are farther away from the hyperplane do not influence its position. Only the support vectors contribute to determining the hyperplane.\n",
    "\n",
    "3. If any non-support vector is removed or its position is changed, it will not affect the optimal hyperplane's position.\n",
    "\n",
    "The role of support vectors becomes more apparent when we realize that the SVM's decision boundary (the hyperplane) is solely determined by these critical points. The margin of the hyperplane is defined by the distance between the support vectors from each class. The SVM aims to maximize this margin while correctly classifying all the training data.\n",
    "\n",
    "In practice, the number of support vectors is often relatively small compared to the total number of training data points. SVMs are designed to find and utilize these critical support vectors to construct the decision boundary effectively, even in high-dimensional spaces or when the data is not linearly separable.\n",
    "\n",
    "By focusing on the support vectors, SVMs are more robust to outliers and generalization performance can be improved. Additionally, SVMs are computationally efficient since the decision boundary is determined by the support vectors' positions, rather than the entire dataset.\n",
    "\n",
    "In summary, support vectors play a central role in SVM by defining the optimal hyperplane and determining the decision boundary. They are the key data points that influence the SVM's learning process, and their positions are critical in achieving good classification performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77a23833-05ff-44be-b08f-d6bb66249359",
   "metadata": {},
   "source": [
    "## Q5. Illustrate with examples and graphs of Hyperplane, Marginal plane, Soft margin and Hard margin in SVM?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9c33c5f-b7ba-4ac5-b278-3afc4412479e",
   "metadata": {},
   "source": [
    "To illustrate the concepts of hyperplane, marginal plane, soft margin, and hard margin in SVM, let's consider a simple 2-dimensional example with two classes: blue squares and red circles.\n",
    "\n",
    "Example Data Points:\n",
    "- Blue Squares: (2, 3), (3, 4), (4, 2)\n",
    "- Red Circles: (1, 1), (3, 1), (4, 4)\n",
    "\n",
    "Let's visualize these data points on a 2D plane:\n",
    "\n",
    "```\n",
    "  |         * (4,4) [Red]\n",
    "  |         \n",
    "  |       * \n",
    "  |    * (3,1) [Red]\n",
    "  |   *    \n",
    "  | * (2,3) [Blue]\n",
    "  |____________________\n",
    "         Feature 1\n",
    "```\n",
    "\n",
    "In SVM, the goal is to find a hyperplane that separates the two classes effectively. A hyperplane in a 2-dimensional space is a line, and in a higher-dimensional space, it would be a plane. The hyperplane is represented by the equation \\( w \\cdot x + b = 0 \\), where \\( w \\) is the weight vector perpendicular to the hyperplane, and \\( b \\) is the bias term.\n",
    "\n",
    "1. **Hard Margin SVM:**\n",
    "In a hard margin SVM, the goal is to find a hyperplane that perfectly separates the two classes with no misclassifications. In other words, there should be no data points inside the margin (on the wrong side). This is only possible when the data is linearly separable. In our example, let's assume the data is linearly separable, and the hard margin SVM finds the following hyperplane:\n",
    "\n",
    "```\n",
    "  |         * (4,4) [Red]\n",
    "  |         \\\n",
    "  |       * \\ \n",
    "  |    *    \\  (3,1) [Red]\n",
    "  |   *      \\\n",
    "  | * (2,3)   \\______________\n",
    "  |____________________\n",
    "         Feature 1\n",
    "```\n",
    "\n",
    "The hyperplane (line) separates the two classes (red and blue) perfectly, and no data points are present inside the margin.\n",
    "\n",
    "2. **Soft Margin SVM:**\n",
    "In real-world scenarios, data may not be perfectly separable due to noise or overlapping classes. In such cases, we use a soft margin SVM, which allows for some misclassifications. The goal of the soft margin SVM is to find a hyperplane that separates the two classes while allowing some data points to be inside the margin. The number of data points allowed inside the margin is controlled by a parameter called \"C.\"\n",
    "\n",
    "Let's consider a soft margin SVM with \\( C = 1 \\) in our example:\n",
    "\n",
    "```\n",
    "  |         * (4,4) [Red]\n",
    "  |         \\\n",
    "  |       * \\ \n",
    "  |    *    \\  (3,1) [Red]\n",
    "  |   *      \\\n",
    "  | * (2,3)   \\______________\n",
    "  |          /   (3,4) [Blue]\n",
    "  |        /   \n",
    "  |      /    \n",
    "  |    /     \n",
    "  |  /       * (4,2) [Blue]\n",
    "  | /       \n",
    "  |/_______\n",
    "```\n",
    "\n",
    "The hyperplane still separates the classes, but it allows some data points to be inside the margin. The data points inside the margin contribute to the SVM's objective function and are called \"slack variables.\" The parameter C controls the trade-off between maximizing the margin and minimizing the classification error. A smaller C allows more data points to be inside the margin, while a larger C penalizes misclassifications more heavily, leading to a narrower margin.\n",
    "\n",
    "It's important to note that the soft margin SVM is more robust to noisy data and can handle non-linearly separable data by using appropriate kernel functions.\n",
    "\n",
    "In summary, the hyperplane is the decision boundary that separates the classes in an SVM. The marginal plane is the region containing the support vectors, and the margin is the space between the marginal planes. Hard margin SVM seeks a perfect separation, while soft margin SVM allows some misclassifications and is more flexible in handling real-world data. The choice between hard and soft margin depends on the nature of the data and the classification task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a2c2c4f-8aad-488e-a701-bed4c5b38b0c",
   "metadata": {},
   "source": [
    "## Q6. SVM Implementation through Iris dataset.\n",
    "\n",
    "\n",
    "- Load the iris dataset from the scikit-learn library and split it into a training set and a testing set\n",
    "- Train a linear SVM classifier on the training set and predict the labels for the testing set\n",
    "- Compute the accuracy of the model on the testing set\n",
    "- Plot the decision boundaries of the trained model using two of the featuresl\n",
    "- Try different values of the regularisation parameter C and see how it affects the performance of\n",
    "the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "59cebacc-4ba3-4cca-9f78-ce515b3a4bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.datasets import load_iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f566caab-9f6d-41cf-9c96-fd394653ddd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1=load_iris()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "50b93f44-b28c-4197-9ff6-28e8ed0e0c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df=pd.DataFrame(df1.data,columns=df1.feature_names)\n",
    "df['target']=df1.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ef431df4-79e3-4e75-856f-6fe3280fa770",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150, 5)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9ced5d3e-5677-44ec-ae6c-197db35ea058",
   "metadata": {},
   "outputs": [],
   "source": [
    "## independent variables and dependent varialbles\n",
    "X=df.iloc[:,:-1]\n",
    "y=df['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "442493c2-c06b-43eb-9a51-95a4410d5c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7eb2dcb2-ea67-4430-8bdf-6e298fe9b1ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.25,random_state=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1a178faa-dba2-4f89-9e76-a1570ca457a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "svc=SVC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6ea886d3-e26f-4699-8592-f77fe204c6b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters={'gamma' : [1, 0.1, 0.01, 0.001, 0.0001],\n",
    "            'C': [0.1, 1, 10, 100, 1000]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d09c689c-06b2-4c86-aefc-cc27df139ec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c2981bff-5967-4fa5-842a-2e04ffe31252",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf=GridSearchCV(SVC(),param_grid=parameters,scoring='accuracy',cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1684a085-458b-437a-a6e5-eea7e73573ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GridSearchCV(cv=5, estimator=SVC(),\n",
       "             param_grid={&#x27;C&#x27;: [0.1, 1, 10, 100, 1000],\n",
       "                         &#x27;gamma&#x27;: [1, 0.1, 0.01, 0.001, 0.0001]},\n",
       "             scoring=&#x27;accuracy&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GridSearchCV</label><div class=\"sk-toggleable__content\"><pre>GridSearchCV(cv=5, estimator=SVC(),\n",
       "             param_grid={&#x27;C&#x27;: [0.1, 1, 10, 100, 1000],\n",
       "                         &#x27;gamma&#x27;: [1, 0.1, 0.01, 0.001, 0.0001]},\n",
       "             scoring=&#x27;accuracy&#x27;)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: SVC</label><div class=\"sk-toggleable__content\"><pre>SVC()</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVC</label><div class=\"sk-toggleable__content\"><pre>SVC()</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "GridSearchCV(cv=5, estimator=SVC(),\n",
       "             param_grid={'C': [0.1, 1, 10, 100, 1000],\n",
       "                         'gamma': [1, 0.1, 0.01, 0.001, 0.0001]},\n",
       "             scoring='accuracy')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "dea3e89d-43bc-4356-b383-40fa5f53e501",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 10, 'gamma': 0.1}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7d807cdc-ec81-490a-b2a2-054550590b02",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "svc=SVC(C=10,gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "c32b3d9f-9049-4765-bcd6-ba30e533d192",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-5 {color: black;background-color: white;}#sk-container-id-5 pre{padding: 0;}#sk-container-id-5 div.sk-toggleable {background-color: white;}#sk-container-id-5 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-5 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-5 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-5 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-5 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-5 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-5 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-5 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-5 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-5 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-5 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-5 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-5 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-5 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-5 div.sk-item {position: relative;z-index: 1;}#sk-container-id-5 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-5 div.sk-item::before, #sk-container-id-5 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-5 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-5 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-5 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-5 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-5 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-5 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-5 div.sk-label-container {text-align: center;}#sk-container-id-5 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-5 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-5\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SVC(C=10, gamma=0.1)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-7\" type=\"checkbox\" checked><label for=\"sk-estimator-id-7\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVC</label><div class=\"sk-toggleable__content\"><pre>SVC(C=10, gamma=0.1)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "SVC(C=10, gamma=0.1)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svc.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "92b5f6e6-e0f7-4256-b411-230a5b76722f",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred=svc.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c28bbca7-64b8-4d9b-96e4-7966856b33de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        11\n",
      "           1       1.00      0.93      0.97        15\n",
      "           2       0.92      1.00      0.96        12\n",
      "\n",
      "    accuracy                           0.97        38\n",
      "   macro avg       0.97      0.98      0.98        38\n",
      "weighted avg       0.98      0.97      0.97        38\n",
      "\n",
      "\n",
      " <function accuracy_score at 0x7fe985ab72e0>\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report,accuracy_score\n",
    "print(classification_report(y_test,y_pred))\n",
    "print('\\n',accuracy_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "2c0d1d28-a784-40e1-9244-68ec7f8a0173",
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42e2f29b-9782-4a8c-9b00-51cd01db4f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot decision boundaries\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, cmap=\"viridis\", edgecolors=\"k\")\n",
    "x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.1), np.arange(y_min, y_max, 0.1))\n",
    "Z = svc.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "Z = Z.reshape(xx.shape)\n",
    "plt.contourf(xx, yy, Z, alpha=0.4, cmap=\"viridis\")\n",
    "plt.xlabel(\"Sepal length (cm)\")\n",
    "plt.ylabel(\"Sepal width (cm)\")\n",
    "plt.title(\"Decision Boundaries of Linear SVM\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "aa47892a-d7b6-4a48-a7b1-94c28a0d2485",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Linear SVM (C=0.01): 0.29\n",
      "Accuracy of Linear SVM (C=0.1): 0.95\n",
      "Accuracy of Linear SVM (C=1.0): 1.00\n",
      "Accuracy of Linear SVM (C=10.0): 1.00\n",
      "Accuracy of Linear SVM (C=100.0): 1.00\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model with different values of C\n",
    "C_values = [0.01, 0.1, 1.0, 10.0, 100.0]\n",
    "for C in C_values:\n",
    "    svc = SVC( C=C)\n",
    "    svc.fit(X_train, y_train)\n",
    "    y_pred=svc.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test,y_pred)\n",
    "    print(f\"Accuracy of Linear SVM (C={C}): {accuracy:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45f1a416-7f11-423f-b163-f7864ea159d6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
