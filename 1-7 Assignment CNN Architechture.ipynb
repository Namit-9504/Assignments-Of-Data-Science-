{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d0f282e1-3ca4-41ae-b70d-8ece74d32de6",
   "metadata": {},
   "source": [
    "## TOPIC: Understanding Pooling and Padding in CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e378f57a-8392-455f-aede-955e36f36b6d",
   "metadata": {},
   "source": [
    "### Q1 Desccire the purpose and benifits oj pooling in CNN."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a53a2e5e-a216-4678-a2c3-dc35b95f98e9",
   "metadata": {},
   "source": [
    "Pooling is a fundamental operation in Convolutional Neural Networks (CNNs) used for feature extraction and spatial dimension reduction. Its purpose and benefits in CNNs can be described as follows:\n",
    "\n",
    "**Purpose of Pooling in CNNs:**\n",
    "\n",
    "1. **Feature Reduction:** One of the primary purposes of pooling is to reduce the spatial dimensions (width and height) of feature maps while preserving their essential information. This reduction helps in controlling the computational complexity of the network and mitigates overfitting by reducing the number of parameters.\n",
    "\n",
    "2. **Translation Invariance:** Pooling helps in achieving translation invariance, which means that the network can recognize the same feature regardless of its exact position in the input image. This is important because in real-world data, objects can appear in different locations.\n",
    "\n",
    "3. **Hierarchical Feature Extraction:** By applying pooling layers at various depths in the network, it allows the network to progressively abstract and represent higher-level features. Lower layers capture low-level features like edges and textures, while higher layers capture more complex features like object parts and objects themselves.\n",
    "\n",
    "**Benefits of Pooling in CNNs:**\n",
    "\n",
    "1. **Dimension Reduction:** Pooling reduces the spatial dimensions of feature maps, making subsequent layers more computationally efficient. Smaller feature maps also reduce memory requirements.\n",
    "\n",
    "2. **Increased Receptive Field:** Pooling effectively enlarges the receptive field of neurons in the subsequent layers. This means that each neuron in a deeper layer looks at a larger region of the input, helping it capture more global features.\n",
    "\n",
    "3. **Robustness to Variations:** Pooling helps the network become more robust to small spatial variations, such as translations, rotations, and scaling of objects within the input data.\n",
    "\n",
    "4. **Noise Tolerance:** Pooling can help reduce the effects of noise in the input data. By summarizing local regions, it focuses on the most prominent features and discards minor variations.\n",
    "\n",
    "5. **Lower Overfitting:** Reduced spatial dimensions and a lower number of parameters make the network less prone to overfitting, as it becomes less sensitive to noise and small variations in the training data.\n",
    "\n",
    "There are different types of pooling techniques used in CNNs, with max pooling and average pooling being the most common. Max pooling selects the maximum value from each pooling region, while average pooling computes the average. These techniques achieve similar goals of spatial dimension reduction and feature extraction, but they differ in their properties and the types of information they preserve. The choice of pooling method depends on the specific problem and architecture of the CNN."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "824c4ec5-e993-474a-8d6e-f058c4b10da2",
   "metadata": {},
   "source": [
    "### Q2 Explain the difference between Min pooling and MaX Pooling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "697d3a57-f2ee-4d0d-8ef1-08aa3fa526d1",
   "metadata": {},
   "source": [
    "It appears there might be a typo in your question (\"Xin pooling\" and \"Xax pooling\"). I assume you are asking about the differences between \"Min pooling\" and \"Max pooling\" in a Convolutional Neural Network (CNN). If that's the case, I can explain the distinctions between these two commonly used pooling operations:\n",
    "\n",
    "**1. Max Pooling:**\n",
    "   - **Operation:** Max pooling is a pooling technique that selects the maximum value from a group of values within a small region (usually a square or rectangular window) of the input feature map.\n",
    "   - **Preservation of Information:** Max pooling focuses on preserving the most prominent features within the pooling region. It tends to capture sharp edges, high-contrast patterns, and the presence of certain specific features.\n",
    "   - **Spatial Dimension Reduction:** After max pooling, the spatial dimensions of the feature map are reduced, which helps control computational complexity and overfitting.\n",
    "   - **Typical Use Cases:** Max pooling is often used in CNN architectures for image classification tasks, where the network needs to identify the most distinctive features of an image.\n",
    "\n",
    "**2. Min Pooling:**\n",
    "   - **Operation:** Min pooling, also known as \"minimum pooling,\" selects the minimum value from a group of values within a pooling region.\n",
    "   - **Preservation of Information:** Min pooling, unlike max pooling, emphasizes the presence of darker or lower-intensity areas within the pooling region. It tends to highlight areas with minimal contrast or low-intensity patterns.\n",
    "   - **Spatial Dimension Reduction:** Like max pooling, min pooling reduces the spatial dimensions of the feature map.\n",
    "   - **Typical Use Cases:** Min pooling is less common than max pooling and may be used in specific scenarios where identifying low-intensity or dark regions in an image is crucial, such as in certain image processing tasks.\n",
    "\n",
    "In summary, the main difference between max pooling and min pooling lies in the type of information they emphasize and preserve within the pooling region. Max pooling focuses on high-intensity features and is more widely used in CNNs for various computer vision tasks. Min pooling, on the other hand, emphasizes low-intensity features, and its use is less common and more task-specific. The choice between these pooling methods depends on the specific characteristics of the data and the goals of the neural network architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e02d2255-c9db-4f97-ae90-fcda04d53013",
   "metadata": {},
   "source": [
    "### Q3 Discuss the concept of padding in CNN and its significance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ed6878e-9a44-407a-bac9-e4f2fd2a10e5",
   "metadata": {},
   "source": [
    "In Convolutional Neural Networks (CNNs), padding is a technique used to control the spatial dimensions of the feature maps (outputs of convolutional layers) and to influence how the convolution operation is applied to the input data. Padding involves adding extra rows and columns of zeros (or other constant values) to the input data before applying convolution. There are two common types of padding:\n",
    "\n",
    "1. **Valid Padding (No Padding):**\n",
    "   - In valid padding (also known as \"no padding\"), no extra rows or columns are added to the input data.\n",
    "   - As a result, the spatial dimensions of the output feature map are smaller than those of the input. Specifically, the output spatial dimensions are determined by the formula:\n",
    "     ```\n",
    "     Output Spatial Dimension = Input Spatial Dimension - Filter Spatial Dimension + 1\n",
    "     ```\n",
    "   - Valid padding is often used when the goal is to reduce the spatial dimensions of the feature maps, which can help control computational complexity and memory usage. However, it may lead to information loss at the edges of the input.\n",
    "\n",
    "2. **Same Padding:**\n",
    "   - In same padding, the necessary number of rows and columns of zeros are added to the input data so that the spatial dimensions of the output feature map are the same as those of the input.\n",
    "   - The padding size (the number of rows and columns of zeros added) depends on the size of the convolutional filter. For a filter of size `FxF` (F is typically an odd number), the padding size is calculated as follows:\n",
    "     ```\n",
    "     Padding Size = (F - 1) / 2\n",
    "     ```\n",
    "   - Same padding is commonly used when you want to preserve the spatial dimensions of the feature maps, making it easier to stack multiple convolutional layers and maintain information at the edges of the input.\n",
    "\n",
    "**Significance of Padding in CNNs:**\n",
    "\n",
    "1. **Preservation of Information:** Padding, especially same padding, helps preserve information at the edges of the input. This is important because edges of an image or spatial features can contain valuable information in many computer vision tasks.\n",
    "\n",
    "2. **Control over Output Size:** Padding allows you to control the spatial dimensions of the output feature maps. Valid padding reduces spatial dimensions, which can be useful for reducing computational load in deeper layers of the network. Same padding preserves spatial dimensions, which is often desirable in the early layers to capture local features.\n",
    "\n",
    "3. **Striding and Overlap:** Padding, in conjunction with striding (how the filter moves across the input), affects the overlap between receptive fields of adjacent neurons in the feature map. It can influence how features are detected and combined.\n",
    "\n",
    "4. **Edge Effects:** Without padding or with insufficient padding, the edges of the input may be underrepresented in the feature maps because the convolution operation is centered on each pixel. Padding helps mitigate this problem.\n",
    "\n",
    "In summary, padding in CNNs plays a significant role in determining the size and content of feature maps after convolution. It allows you to control how spatial information is retained and can be crucial for achieving desired results in various computer vision tasks. The choice of padding type depends on the specific architecture, task, and goals of your CNN."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98490ff4-4514-4f59-9095-0675ba941628",
   "metadata": {},
   "source": [
    "### Q4 Compare and contrast zero-padding and valid-padding in terms of their effects on the output feature map size."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "904726df-4a34-466e-9a49-0544fe0e8f0d",
   "metadata": {},
   "source": [
    "In the context of convolutional neural networks (CNNs), space and contrast (zero) padding refer to two different techniques used to control the size and content of the output feature maps when applying convolution operations. These techniques have distinct effects on the output feature map size:\n",
    "\n",
    "**1. Space (or Zero) Padding:**\n",
    "   - **Operation:** Space padding, often referred to as zero-padding, involves adding rows and columns of zeros (or a constant value) around the input data before performing convolution.\n",
    "   - **Effect on Output Size:** Space padding increases the spatial dimensions of the input, which in turn affects the size of the output feature map. The padding size determines the extent to which the output size increases. Specifically, if you add `P` rows/columns of padding to each side of the input, the output size increases by `2*P` in both dimensions.\n",
    "   - **Purpose:** The primary purpose of space padding is to preserve the spatial dimensions of the input, especially at the edges. It helps to retain information in those regions during convolution.\n",
    "   - **Common Use Case:** Space padding is frequently used when you want the output feature map size to match the input size or when you want to control the receptive field of neurons in deeper layers of the network.\n",
    "\n",
    "**2. Valid Padding:**\n",
    "   - **Operation:** Valid padding, also known as \"no padding,\" does not add any extra rows or columns around the input. The convolution operation is applied directly to the input data.\n",
    "   - **Effect on Output Size:** Valid padding reduces the spatial dimensions of the input, which results in smaller output feature maps compared to the input. The reduction in size is determined by the size of the convolutional filter.\n",
    "   - **Purpose:** The primary purpose of valid padding is to reduce the spatial dimensions of the feature maps. This reduction is useful for controlling computational complexity, memory usage, and achieving spatial abstraction in deeper layers.\n",
    "   - **Common Use Case:** Valid padding is often used in deeper layers of the network when the goal is to reduce spatial dimensions progressively.\n",
    "\n",
    "Here's a simplified formula to understand the relationship between padding, filter size, and the effect on output size:\n",
    "\n",
    "```\n",
    "Output Spatial Dimension = (Input Spatial Dimension + 2*Padding - Filter Spatial Dimension) / Stride + 1\n",
    "```\n",
    "\n",
    "In this formula, `Padding` is the number of rows/columns of padding added to the input, `Filter Spatial Dimension` is the size of the convolutional filter, and `Stride` is the step size at which the filter is moved across the input.\n",
    "\n",
    "In summary, the choice between space padding and valid padding in a CNN depends on your specific goals. Space padding helps preserve spatial information, especially at the edges, while valid padding reduces spatial dimensions, which can be useful for computational efficiency and feature abstraction. The choice may also depend on the layer's position within the network and the architectural design."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "830449ee-d0d2-4094-9056-68cd2ecafca8",
   "metadata": {},
   "source": [
    "## TOPIC: Exploring LeNet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13344468-d74e-45be-a979-f70f912bfec7",
   "metadata": {},
   "source": [
    "### Q1 Provide a brief overview of LeNet-5 acchitecture."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e5f6bae-a1e1-4a46-9601-cf1e7f9f802a",
   "metadata": {},
   "source": [
    "LeNet-5 is a classic and influential convolutional neural network (CNN) architecture that was developed by Yann LeCun and his colleagues in the late 1990s. It played a pivotal role in the advancement of deep learning and was primarily designed for handwritten digit recognition tasks, specifically for recognizing characters on checks and other financial documents. Here's an overview of the LeNet-5 architecture:\n",
    "\n",
    "**Layer 1: Input Layer**\n",
    "- Input images are typically grayscale and have dimensions of 32x32 pixels.\n",
    "\n",
    "**Layer 2: Convolutional Layer (C1)**\n",
    "- Convolutional operation with a 5x5 kernel.\n",
    "- 6 feature maps (also referred to as channels or filters).\n",
    "- Activation function: Sigmoid.\n",
    "- Subsampling (Pooling): 2x2 max-pooling with a stride of 2.\n",
    "\n",
    "**Layer 3: Convolutional Layer (C3)**\n",
    "- Convolutional operation with a 5x5 kernel.\n",
    "- 16 feature maps.\n",
    "- Activation function: Sigmoid.\n",
    "- Subsampling (Pooling): 2x2 max-pooling with a stride of 2.\n",
    "\n",
    "**Layer 4: Fully Connected Layer (F4)**\n",
    "- Flattening of the feature maps from the previous layer.\n",
    "- 120 neurons.\n",
    "- Activation function: Sigmoid.\n",
    "\n",
    "**Layer 5: Fully Connected Layer (F5)**\n",
    "- 84 neurons.\n",
    "- Activation function: Sigmoid.\n",
    "\n",
    "**Layer 6: Output Layer (Output)**\n",
    "- The output layer consists of 10 neurons (one for each digit, 0-9).\n",
    "- Activation function: Softmax.\n",
    "- This layer produces a probability distribution over the possible digit classes.\n",
    "\n",
    "**Key Points:**\n",
    "\n",
    "1. **Convolutional Layers:** LeNet-5 introduced the concept of using convolutional layers for feature extraction. These layers learn to detect various low-level and mid-level features, such as edges, corners, and textures.\n",
    "\n",
    "2. **Pooling Layers:** The use of max-pooling layers after convolutional layers helps reduce spatial dimensions and extract the most important information from the feature maps.\n",
    "\n",
    "3. **Fully Connected Layers:** LeNet-5 ends with two fully connected layers (F4 and F5), which combine the extracted features to make predictions.\n",
    "\n",
    "4. **Activation Functions:** Sigmoid activation functions were used in the hidden layers, which was common at the time of its development. However, modern CNN architectures often use ReLU (Rectified Linear Unit) activations.\n",
    "\n",
    "5. **Softmax Output:** The softmax function in the output layer converts the final layer's scores into a probability distribution, making it suitable for multiclass classification tasks like digit recognition.\n",
    "\n",
    "6. **Overall Simplicity:** LeNet-5 is relatively small and simple compared to modern CNN architectures. It served as a foundational model for more complex and deeper networks like AlexNet, VGG, and ResNet.\n",
    "\n",
    "While LeNet-5 was originally designed for digit recognition, its principles and architectural elements have influenced the development of more advanced CNNs used in a wide range of computer vision tasks, including image classification, object detection, and image segmentation. It remains an important milestone in the history of deep learning and convolutional neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9c4b3a04-64c1-464a-8a97-f53b7716e5c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 222, 222, 32)      896       \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2  (None, 111, 111, 32)      0         \n",
      " D)                                                              \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 109, 109, 64)      18496     \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPoolin  (None, 54, 54, 64)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 186624)            0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 120)               22395000  \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 60)                7260      \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 61        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 22421713 (85.53 MB)\n",
      "Trainable params: 22421713 (85.53 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense ,Flatten , MaxPooling2D , Conv2D\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(filters=32,kernel_size=(3,3),activation='relu',input_shape=(224,224,3)))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "model.add(Conv2D(filters=64,kernel_size=(3,3),activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(120,activation='relu'))\n",
    "model.add(Dense(60,activation='relu'))\n",
    "model.add(Dense(1,activation='sigmoid'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db8378a1-1ab2-457e-9319-43d9a5b84a78",
   "metadata": {},
   "source": [
    "### Q3 Describe the key components of LeNet-5 and their respective purpose."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "281730b0-5a8f-44c9-a2c2-a7f2157c7928",
   "metadata": {},
   "source": [
    "LeNet-5, a pioneering Convolutional Neural Network (CNN) architecture developed by Yann LeCun and his colleagues, consists of several key components, each with its specific purpose in the network. Here's a detailed description of the key components of LeNet-5 and their respective purposes:\n",
    "\n",
    "1. **Input Layer:**\n",
    "   - **Purpose:** The input layer receives the raw image data as input.\n",
    "   - **Details:** LeNet-5 typically takes grayscale images with dimensions of 32x32 pixels as input.\n",
    "\n",
    "2. **Convolutional Layers (C1 and C3):**\n",
    "   - **Purpose:** These layers perform feature extraction by applying convolutional operations.\n",
    "   - **Details:** \n",
    "     - C1: The first convolutional layer with a 5x5 kernel extracts low-level features like edges and textures. It has 6 feature maps.\n",
    "     - C3: The second convolutional layer also has a 5x5 kernel and extracts higher-level features. It has 16 feature maps.\n",
    "\n",
    "3. **Activation Functions (Sigmoid):**\n",
    "   - **Purpose:** Sigmoid activation functions introduce non-linearity to the network, enabling it to learn complex relationships in the data.\n",
    "   - **Details:** Sigmoid activation functions are used in the convolutional and fully connected layers (F4 and F5). While modern CNNs often use ReLU (Rectified Linear Unit) activations for improved training speed, LeNet-5 used sigmoid activations at the time of its development.\n",
    "\n",
    "4. **Subsampling Layers (Pooling, S2 and S4):**\n",
    "   - **Purpose:** These layers reduce spatial dimensions and downsample the feature maps, focusing on the most salient information.\n",
    "   - **Details:** \n",
    "     - S2: After C1, a 2x2 max-pooling layer with a stride of 2 is applied.\n",
    "     - S4: After C3, another 2x2 max-pooling layer with a stride of 2 is applied.\n",
    "\n",
    "5. **Fully Connected Layers (F4 and F5):**\n",
    "   - **Purpose:** These layers combine the extracted features and make higher-level abstractions.\n",
    "   - **Details:** \n",
    "     - F4: The first fully connected layer with 120 neurons.\n",
    "     - F5: The second fully connected layer with 84 neurons.\n",
    "\n",
    "6. **Output Layer (Output):**\n",
    "   - **Purpose:** The output layer produces the final predictions and probabilities for different classes.\n",
    "   - **Details:** The output layer consists of 10 neurons, one for each digit class (0-9). The softmax activation function is used to generate a probability distribution over these classes.\n",
    "\n",
    "7. **Softmax Activation:**\n",
    "   - **Purpose:** The softmax activation function in the output layer converts the raw scores into class probabilities, facilitating multiclass classification.\n",
    "   - **Details:** It ensures that the sum of the predicted probabilities for all classes is equal to 1.\n",
    "\n",
    "8. **Loss Function:**\n",
    "   - **Purpose:** The loss function (commonly cross-entropy loss) is used to measure the error between the predicted probabilities and the true labels.\n",
    "   - **Details:** The goal is to minimize this loss during training to improve the model's accuracy.\n",
    "\n",
    "In summary, LeNet-5 comprises convolutional layers for feature extraction, subsampling layers for spatial dimension reduction, fully connected layers for abstraction, and an output layer for classification. Sigmoid activations and the softmax function are used for non-linearity and probability estimation. While LeNet-5 is relatively simple by today's standards, it laid the foundation for modern CNN architectures and demonstrated the effectiveness of deep learning for image recognition tasks, particularly handwritten digit recognition."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c99d4406-ed15-441a-abc7-e0db7ec38f02",
   "metadata": {},
   "source": [
    "### Q3 Discuss the advantages and limitations of LeNet-5 in the context oj image classification tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3966166c-0c64-4e2b-8de6-d5ef22ba42ff",
   "metadata": {},
   "source": [
    "LeNet-5, as one of the pioneering convolutional neural network (CNN) architectures, has both advantages and limitations, especially in the context of image classification tasks:\n",
    "\n",
    "**Advantages of LeNet-5:**\n",
    "\n",
    "1. **Effective Feature Extraction:** LeNet-5 effectively extracts hierarchical features from input images through its convolutional layers. This ability to learn and capture features at different levels of abstraction is crucial for image classification.\n",
    "\n",
    "2. **Preservation of Spatial Information:** The use of max-pooling layers with small filter sizes helps preserve important spatial information while reducing the spatial dimensions of the feature maps. This is particularly useful for retaining local features.\n",
    "\n",
    "3. **Training with Limited Data:** LeNet-5 demonstrated that deep learning can be effective even with relatively small datasets. This finding was essential at a time when large labeled datasets were not as readily available as they are today.\n",
    "\n",
    "4. **Inspiration for Future Architectures:** LeNet-5 served as a foundational model that inspired subsequent CNN architectures, including AlexNet, VGG, and more. It introduced key concepts like convolutional layers, subsampling, and fully connected layers that became standard in CNN design.\n",
    "\n",
    "5. **Multiclass Classification:** LeNet-5 demonstrated the effectiveness of CNNs for multiclass classification problems, particularly in recognizing handwritten digits, which laid the groundwork for more complex image classification tasks.\n",
    "\n",
    "**Limitations of LeNet-5:**\n",
    "\n",
    "1. **Limited Capacity:** LeNet-5 has a relatively small architecture by modern standards. It may struggle with more complex image recognition tasks that require capturing a wide range of intricate details.\n",
    "\n",
    "2. **Sigmoid Activation Functions:** LeNet-5 uses sigmoid activation functions, which suffer from vanishing gradient problems and can slow down training. Modern CNNs typically use rectified linear units (ReLU) for faster convergence.\n",
    "\n",
    "3. **Small Input Size:** LeNet-5 was designed for 32x32 pixel grayscale images. While it was effective for digit recognition, it may not perform as well on larger or more detailed images.\n",
    "\n",
    "4. **Not Suitable for Large Datasets:** While LeNet-5 demonstrated the power of deep learning with small datasets, it may not be the best choice for tasks that have access to large datasets. Modern architectures, with more capacity, can take better advantage of big data.\n",
    "\n",
    "5. **Lack of Convolutional Depth:** LeNet-5 has only two convolutional layers. Deeper networks have shown greater ability to learn complex features and hierarchies.\n",
    "\n",
    "In summary, LeNet-5 was a groundbreaking architecture that paved the way for deep learning in computer vision. It showcased the potential of CNNs for image classification tasks, particularly in the early days of deep learning. However, its limitations, such as small capacity and the use of sigmoid activations, make it less suitable for state-of-the-art image classification tasks on large, complex datasets. Modern CNNs have built upon the concepts introduced by LeNet-5 and have achieved significant advances in image classification accuracy and capability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b91ebdc-b341-47e9-8ef3-3c5f32747748",
   "metadata": {},
   "source": [
    "### Q4 Implement LeNet-5 using a deep learning framework of your choice (e.g., TensorFlow, PyTorch) and train it on a publicly available dataset (e.g., MNIST). Evaluate its pecjocmance and provide insights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d7a42e48-0927-41f4-bfe3-ecbe02f47101",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_shape:  (28, 28, 1)\n",
      "x_train shape: (60000, 28, 28, 1)\n",
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_6 (Conv2D)           (None, 28, 28, 32)        320       \n",
      "                                                                 \n",
      " max_pooling2d_6 (MaxPoolin  (None, 14, 14, 32)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_7 (Conv2D)           (None, 14, 14, 64)        18496     \n",
      "                                                                 \n",
      " max_pooling2d_7 (MaxPoolin  (None, 7, 7, 64)          0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " flatten_3 (Flatten)         (None, 3136)              0         \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 120)               376440    \n",
      "                                                                 \n",
      " dense_10 (Dense)            (None, 60)                7260      \n",
      "                                                                 \n",
      " dense_11 (Dense)            (None, 10)                610       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 403126 (1.54 MB)\n",
      "Trainable params: 403126 (1.54 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "\n",
      "Epoch 1: val_loss improved from inf to 0.04739, saving model to model.weights.best.hdf5\n",
      "938/938 - 12s - loss: 0.1393 - accuracy: 0.9587 - val_loss: 0.0474 - val_accuracy: 0.9857 - 12s/epoch - 13ms/step\n",
      "Epoch 2/10\n",
      "\n",
      "Epoch 2: val_loss improved from 0.04739 to 0.03625, saving model to model.weights.best.hdf5\n",
      "938/938 - 11s - loss: 0.0433 - accuracy: 0.9869 - val_loss: 0.0362 - val_accuracy: 0.9877 - 11s/epoch - 12ms/step\n",
      "Epoch 3/10\n",
      "\n",
      "Epoch 3: val_loss improved from 0.03625 to 0.02776, saving model to model.weights.best.hdf5\n",
      "938/938 - 11s - loss: 0.0295 - accuracy: 0.9910 - val_loss: 0.0278 - val_accuracy: 0.9904 - 11s/epoch - 12ms/step\n",
      "Epoch 4/10\n",
      "\n",
      "Epoch 4: val_loss did not improve from 0.02776\n",
      "938/938 - 11s - loss: 0.0212 - accuracy: 0.9934 - val_loss: 0.0295 - val_accuracy: 0.9910 - 11s/epoch - 12ms/step\n",
      "Epoch 5/10\n",
      "\n",
      "Epoch 5: val_loss did not improve from 0.02776\n",
      "938/938 - 11s - loss: 0.0151 - accuracy: 0.9954 - val_loss: 0.0314 - val_accuracy: 0.9891 - 11s/epoch - 12ms/step\n",
      "Epoch 6/10\n",
      "\n",
      "Epoch 6: val_loss did not improve from 0.02776\n",
      "938/938 - 11s - loss: 0.0115 - accuracy: 0.9964 - val_loss: 0.0349 - val_accuracy: 0.9901 - 11s/epoch - 12ms/step\n",
      "Epoch 7/10\n",
      "\n",
      "Epoch 7: val_loss did not improve from 0.02776\n",
      "938/938 - 11s - loss: 0.0083 - accuracy: 0.9974 - val_loss: 0.0331 - val_accuracy: 0.9903 - 11s/epoch - 12ms/step\n",
      "Epoch 8/10\n",
      "\n",
      "Epoch 8: val_loss did not improve from 0.02776\n",
      "938/938 - 12s - loss: 0.0066 - accuracy: 0.9980 - val_loss: 0.0343 - val_accuracy: 0.9910 - 12s/epoch - 13ms/step\n",
      "Epoch 9/10\n",
      "\n",
      "Epoch 9: val_loss did not improve from 0.02776\n",
      "938/938 - 11s - loss: 0.0049 - accuracy: 0.9985 - val_loss: 0.0398 - val_accuracy: 0.9901 - 11s/epoch - 12ms/step\n",
      "Epoch 10/10\n",
      "\n",
      "Epoch 10: val_loss did not improve from 0.02776\n",
      "938/938 - 11s - loss: 0.0036 - accuracy: 0.9990 - val_loss: 0.0387 - val_accuracy: 0.9903 - 11s/epoch - 12ms/step\n",
      "Test accuracy: 99.0400%\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Dense , Flatten , MaxPooling2D , Conv2D , Dropout\n",
    "from keras.models import Sequential\n",
    "from keras.datasets import mnist\n",
    "from keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "\n",
    "(X_train,y_train),(X_test,y_test)=mnist.load_data()\n",
    "\n",
    "X_train = X_train.astype('float32')/255\n",
    "X_test = X_test.astype('float32')/255\n",
    "\n",
    "y_train = to_categorical(y_train,num_classes=10)\n",
    "y_test = to_categorical(y_test,num_classes=10)\n",
    "\n",
    "\n",
    "img_rows, img_cols = 28, 28\n",
    "\n",
    "X_train = X_train.reshape(X_train.shape[0], img_rows, img_cols, 1)\n",
    "X_test = X_test.reshape(X_test.shape[0], img_rows, img_cols, 1)\n",
    "input_shape = (img_rows, img_cols, 1)\n",
    "\n",
    "print('input_shape: ', input_shape)\n",
    "print('x_train shape:', X_train.shape)\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(filters=32,kernel_size=(3,3),activation='relu',padding='same',input_shape=(28,28,1)))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "model.add(Conv2D(filters=64,kernel_size=(3,3),activation='relu',padding='same'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(120,activation='tanh'))\n",
    "model.add(Dense(60,activation='tanh'))\n",
    "model.add(Dense(10,activation='softmax'))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='rmsprop',metrics=['accuracy'])\n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath='model.weights.best.hdf5', verbose=1,save_best_only=True)\n",
    "\n",
    "hist = model.fit(X_train, y_train, batch_size=64, epochs=10,validation_data=(X_test, y_test), callbacks=[checkpointer] ,verbose=2, shuffle=True)\n",
    "\n",
    "\n",
    "model.load_weights('model.weights.best.hdf5')\n",
    "\n",
    "score = model.evaluate(X_test, y_test, verbose=0)\n",
    "accuracy = 100*score[1]\n",
    "\n",
    "\n",
    "print('Test accuracy: %.4f%%' % accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73adb0e9-6369-44f5-bb79-eaf24e7b1342",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
