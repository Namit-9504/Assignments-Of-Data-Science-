{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dc4f143b-3867-44de-a965-e79a2b1fe8e0",
   "metadata": {},
   "source": [
    "## Q1. What is the purpose of grid search cv in machine learning, and how does it work?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7568f31d-37ec-4d81-bc8c-0958fc7c0fc2",
   "metadata": {},
   "source": [
    "Grid search CV (cross-validation) is a technique used to find the best hyperparameters for a machine learning model. Hyperparameters are the settings of the model that are not learned from the data. For example, the learning rate of a neural network is a hyperparameter.\n",
    "\n",
    "Grid search CV works by creating a grid of hyperparameter values and then evaluating the model on each combination of hyperparameter values. The hyperparameter combination that results in the best performance on the validation data is chosen as the best hyperparameters for the model.\n",
    "\n",
    "For example, let's say we are building a logistic regression model to predict whether a customer will click on an ad. We have two hyperparameters to tune: the learning rate and the regularization strength. We can create a grid of hyperparameter values as follows:\n",
    "\n",
    "learning_rate = [0.001, 0.005, 0.01, 0.05, 0.1]\n",
    "\n",
    "regularization_strength = [0.001, 0.005, 0.01, 0.05, 0.1]\n",
    "\n",
    "This gives us 25 different combinations of hyperparameter values to evaluate. We can then train the logistic regression model on the training data for each combination of hyperparameter values and evaluate the model on the validation data. The combination of hyperparameter values that results in the best performance on the validation data is chosen as the best hyperparameters for the model.\n",
    "\n",
    "Grid search CV is a powerful technique for finding the best hyperparameters for a machine learning model. However, it can be computationally expensive, especially if there are a large number of hyperparameters to tune.\n",
    "\n",
    "Here are some additional things to keep in mind about grid search CV:\n",
    "\n",
    "- The validation set: The performance of the model should be evaluated on a validation set that was not used to train the model. This will help to ensure that the model is not overfitting the training data.\n",
    "- The number of hyperparameters: The number of hyperparameters that need to be tuned can have a significant impact on the computational cost of grid search CV. If there are a large number of hyperparameters, grid search CV can be very time-consuming.\n",
    "- The stopping criteria: Grid search CV can be stopped early if the performance of the model does not improve on the validation set after a certain number of iterations. This can help to reduce the computational cost of grid search CV.`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22651f35-4000-4905-b7a1-658915aec714",
   "metadata": {},
   "source": [
    "## Q2. Describe the difference between grid search cv and randomize search cv, and when might you choose one over the other?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76767dda-be37-4dfb-afbe-4cd0c5050862",
   "metadata": {},
   "source": [
    " Grid search CV and random search CV are both techniques used to find the best hyperparameters for a machine learning model. However, they have different approaches.\n",
    "\n",
    "Grid search CV exhaustively searches a grid of hyperparameter values. This means that it evaluates all possible combinations of hyperparameter values. For example, if you have 2 hyperparameters with 5 possible values each, grid search CV will evaluate 25 different combinations of hyperparameter values.\n",
    "\n",
    "Random search CV randomly samples hyperparameter values from a distribution. This means that it does not evaluate all possible combinations of hyperparameter values. For example, if you have 2 hyperparameters with 5 possible values each, random search CV might sample 10 different combinations of hyperparameter values.\n",
    "\n",
    "The main difference between grid search CV and random search CV is the computational cost. Grid search CV is more computationally expensive than random search CV because it evaluates all possible combinations of hyperparameter values.\n",
    "\n",
    "When to choose one over the other:\n",
    "\n",
    "Grid search CV: If you have a small number of hyperparameters to tune and you want to be sure that you have found the best hyperparameters, then grid search CV is a good option.\n",
    "\n",
    "Random search CV: If you have a large number of hyperparameters to tune or if you are short on computational resources, then random search CV is a good option"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ec1bd19-3686-447c-b5b3-48888f25b9cc",
   "metadata": {},
   "source": [
    "## Q3. What is data leakage, and why is it a problem in machine learning? Provide an example."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f17fbbd2-af3c-4197-b786-a1e2af0f4057",
   "metadata": {},
   "source": [
    "\n",
    "Data leakage is a problem in machine learning that occurs when data from the test set is used to train the model. This can happen in a number of ways, such as:\n",
    "\n",
    "- Using the test set to select features or hyperparameters\n",
    "- Using the test set to evaluate the model during training\n",
    "- Using the test set to debug the model\n",
    "\n",
    "Data leakage can lead to the model overfitting the test set, which means that the model will perform well on the test set but will not generalize well to new data.\n",
    "\n",
    "For example, let's say you are building a model to predict whether a customer will click on an ad. You have a dataset of historical data that includes the customer's click history. You also have a test set of data that includes the customer's click history for the past week.\n",
    "\n",
    "If you use the test set to select features or hyperparameters, the model will be trained on data that it will not see in the future. This can lead to the model overfitting the test set and not generalizing well to new data.\n",
    "\n",
    "Here is another example. Let's say you are building a model to predict the price of a house. You have a dataset of historical data that includes the price of houses that have sold in the past. You also have a test set of data that includes the price of houses that are currently on the market.\n",
    "\n",
    "If you use the test set to evaluate the model during training, the model will be trained on data that it will not see in the future. This can lead to the model overfitting the test set and not generalizing well to new data.\n",
    "\n",
    "To avoid data leakage, it is important to keep the test set separate from the training set. The test set should only be used to evaluate the model once it has been trained.\n",
    "\n",
    "Here are some tips for avoiding data leakage:\n",
    "\n",
    "- Use a holdout set: A holdout set is a set of data that is not used for training or evaluation. The holdout set can be used to evaluate the model once it has been trained.\n",
    "- Train the model on a random subset of the data: This will help to ensure that the model is not trained on data that is too similar to the test set.\n",
    "- Use a cross-validation technique: Cross-validation is a technique that evaluates the model on multiple subsets of the data. This can help to identify any data leakage problems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "596acfd1-efbf-480f-85ff-7f7ca9a87492",
   "metadata": {},
   "source": [
    "## Q4. How can you prevent data leakage when building a machine learning model?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93727b80-1091-4b10-adf3-bdbd51619972",
   "metadata": {},
   "source": [
    "Here are some tips on how to prevent data leakage when building a machine learning model:\n",
    "\n",
    "Keep the training set and test set separate. The training set should only be used to train the model, and the test set should only be used to evaluate the model once it has been trained.\n",
    "Use a holdout set. A holdout set is a set of data that is not used for training or evaluation. The holdout set can be used to evaluate the model once it has been trained, and it can also be used to identify any data leakage problems.\n",
    "\n",
    "Use a cross-validation technique. Cross-validation is a technique that evaluates the model on multiple subsets of the data. This can help to identify any data leakage problems.\n",
    "\n",
    "Be careful about how you use the test set. The test set should only be used to evaluate the model once it has been trained. Do not use the test set to select features or hyperparameters, or to debug the model.\n",
    "\n",
    "Use a version control system. This will help you to track changes to the data and the model, and it will make it easier to identify any data leakage problems.\n",
    "\n",
    "Here are some additional things to keep in mind about data leakage:\n",
    "\n",
    "Data leakage can be difficult to detect. It is often not obvious that data leakage has occurred, and it can be difficult to identify the source of the problem.\n",
    "\n",
    "Data leakage can have a significant impact on the model's performance. If the model is trained on data that it will not see in the future, it will not be able to generalize well to new data.\n",
    "By following these tips, you can help to prevent data leakage and build machine learning models that are more accurate and reliable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dbf3723-3b13-4e73-b2d8-781781bd0252",
   "metadata": {},
   "source": [
    "## Q5. What is a confusion matrix, and what does it tell you about the performance of a classification model?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22d4088f-edba-433f-a89c-1ac92f007b93",
   "metadata": {},
   "source": [
    "\n",
    "A confusion matrix is a table that is used to evaluate the performance of a classification model. It shows the number of predictions that the model made correctly and incorrectly, for each class.\n",
    "\n",
    "A confusion matrix is a useful tool for understanding the strengths and weaknesses of a classification model. It can help to identify the types of errors that the model is making, and it can also be used to compare the performance of different models.\n",
    "\n",
    "The confusion matrix is a square table with two dimensions: the predicted class and the true class. The predicted class is the class that the model predicts, and the true class is the actual class of the data point.\n",
    "\n",
    "The confusion matrix is typically divided into four quadrants:\n",
    "\n",
    "- True Positives (TP): The number of data points that were correctly classified as the positive class.\n",
    "- True Negatives (TN): The number of data points that were correctly classified as the negative class.\n",
    "- False Positives (FP): The number of data points that were incorrectly classified as the positive class.\n",
    "- False Negatives (FN): The number of data points that were incorrectly classified as the negative class.\n",
    "\n",
    "The confusion matrix can be used to calculate a number of metrics that measure the performance of a classification model. These metrics include:\n",
    "\n",
    "- Accuracy: The accuracy is the percentage of data points that were correctly classified. It is calculated by dividing the sum of the true positives and true negatives by the total number of data points.\n",
    "- Precision: The precision is the percentage of data points that were classified as positive that were actually positive. It is calculated by dividing the true positives by the sum of the true positives and false positives.\n",
    "- Recall: The recall is the percentage of data points that were actually positive that were classified as positive. It is calculated by dividing the true positives by the sum of the true positives and false negatives.\n",
    "\n",
    "The confusion matrix is a valuable tool for evaluating the performance of a classification model. It can help to identify the strengths and weaknesses of the model, and it can also be used to compare the performance of different models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ff7bb2b-4355-40cf-9f15-e9e043ef111b",
   "metadata": {},
   "source": [
    "## Q6. Explain the difference between precision and recall in the context of a confusion matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45af80ae-42fd-4d62-963e-66843b3d514a",
   "metadata": {},
   "source": [
    "Precision and recall are two metrics that are used to evaluate the performance of a binary classification model. A binary classification model is a model that predicts whether a data point belongs to one of two classes.\n",
    "\n",
    "Precision is the fraction of positive predictions that are actually positive. It is calculated by dividing the true positives by the sum of the true positives and false positives.\n",
    "\n",
    "Recall is the fraction of actual positives that are correctly predicted as positive. It is calculated by dividing the true positives by the sum of the true positives and false negatives.\n",
    "\n",
    "In the context of a confusion matrix, precision is the ratio of the TP to the sum of TP and FP, while recall is the ratio of the TP to the sum of TP and FN.\n",
    "\n",
    "For example, let's say we have a confusion matrix with the following values:\n",
    "\n",
    "Predicted\tTrue\n",
    "Positive\t10\n",
    "Negative\t2\n",
    "\n",
    "\n",
    "The precision for this model is 10/(10+5) = 0.67, and the recall for this model is 10/(10+2) = 0.83.\n",
    "\n",
    "Precision and recall are both important metrics for evaluating the performance of a binary classification model. Precision measures how accurate the model's predictions are, while recall measures how complete the model's predictions are.\n",
    "\n",
    "In general, a high precision model will have few false positives, while a high recall model will have few false negatives. The best model will have a high precision and recall.\n",
    "\n",
    "However, there is often a trade-off between precision and recall. A model can be tuned to have a high precision, but this may come at the cost of recall. Conversely, a model can be tuned to have a high recall, but this may come at the cost of precision.\n",
    "\n",
    "The best approach to tuning a model will depend on the specific application. For example, if the application requires that the model minimize false positives, then the model should be tuned for high precision. Conversely, if the application requires that the model minimize false negatives, then the model should be tuned for high recall."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1854a71c-10b0-4d29-a985-929c65acff74",
   "metadata": {},
   "source": [
    "## Q7. How can you interpret a confusion matrix to determine which types of errors your model is making?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bd5d467-f0c4-4c60-847e-d80832508323",
   "metadata": {},
   "source": [
    "A confusion matrix can help you to interpret the errors that your model is making by showing you the number of predictions that the model made correctly and incorrectly, for each class.\n",
    "\n",
    "The confusion matrix is a square table with two dimensions: the predicted class and the true class. The predicted class is the class that the model predicts, and the true class is the actual class of the data point.\n",
    "\n",
    "The confusion matrix is typically divided into four quadrants:\n",
    "\n",
    "- True Positives (TP): The number of data points that were correctly classified as the positive class.\n",
    "- True Negatives (TN): The number of data points that were correctly classified as the negative class.\n",
    "- False Positives (FP): The number of data points that were incorrectly classified as the positive class.\n",
    "- False Negatives (FN): The number of data points that were incorrectly classified as the negative class.\n",
    "\n",
    "\n",
    "By looking at the confusion matrix, you can see which types of errors your model is making. For example, if the model has a high number of false positives, then it is predicting that the data points are positive when they are actually negative. This can be a problem if the application requires that the model minimize false positives.\n",
    "\n",
    "Conversely, if the model has a high number of false negatives, then it is predicting that the data points are negative when they are actually positive. This can be a problem if the application requires that the model minimize false negatives.\n",
    "\n",
    "The confusion matrix can also help you to identify the types of data points that the model is having difficulty with. For example, if the model has a high number of false positives for a particular class, then it may be that the model is not able to distinguish between that class and another class.\n",
    "\n",
    "By understanding the types of errors that your model is making, you can take steps to improve the model's performance. For example, you may need to adjust the model's hyperparameters or retrain the model on a different dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9421c28-2de5-422e-a787-8b416fa4d21a",
   "metadata": {},
   "source": [
    "## Q8. What are some common metrics that can be derived from a confusion matrix, and how are they calculated?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8880a8c6-a1e7-48db-a3e7-52e12e736b87",
   "metadata": {},
   "source": [
    ". Here are some common metrics that can be derived from a confusion matrix, and how they are calculated:\n",
    "\n",
    "Accuracy: The accuracy is the percentage of data points that were correctly classified. It is calculated by dividing the sum of the true positives and true negatives by the total number of data points.\n",
    "\n",
    "Accuracy = (TP + TN) / (TP + TN + FP + FN)\n",
    "\n",
    "Precision: The precision is the percentage of data points that were classified as positive that were actually positive. It is calculated by dividing the true positives by the sum of the true positives and false positives.\n",
    "\n",
    "Precision = TP / (TP + FP)\n",
    "\n",
    "Recall: The recall is the percentage of data points that were actually positive that were classified as positive. \n",
    "\n",
    "It is calculated by dividing the true positives by the sum of the true positives and false negatives.\n",
    "Recall = TP / (TP + FN)\n",
    "\n",
    "F1 Score: The F1 score is a weighted average of the precision and recall. It is calculated by taking the harmonic mean of the precision and recall.\n",
    "\n",
    "F1 Score = 2 * (Precision * Recall) / (Precision + Recall)\n",
    "\n",
    "Specificity: The specificity is the percentage of data points that were classified as negative that were actually negative. It is calculated by dividing the true negatives by the sum of the true negatives and false positives.\n",
    "\n",
    "Specificity = TN / (TN + FP)\n",
    "\n",
    "MCC: The Matthews correlation coefficient (MCC) is a metric that takes into account all four quadrants of the confusion matrix. It is a more robust metric than accuracy, precision, and recall, and it is often used in imbalanced datasets.\n",
    "\n",
    "MCC = (TP * TN - FP * FN) / sqrt((TP + FP) * (TP + FN) * (TN + FP) * (TN + FN))\n",
    "\n",
    "These are just some of the common metrics that can be derived from a confusion matrix. The specific metrics that are most appropriate for a particular application will depend on the specific goals of the application."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc30bc34-0761-4874-9106-fd9967ef5c64",
   "metadata": {},
   "source": [
    "## Q9. What is the relationship between the accuracy of a model and the values in its confusion matrix?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed629fa7-ecb9-4bdd-b000-e7a8aeca36d0",
   "metadata": {},
   "source": [
    "\n",
    "The accuracy of a model is the percentage of data points that were correctly classified. It is calculated by dividing the sum of the true positives and true negatives by the total number of data points.\n",
    "\n",
    "The values in a confusion matrix represent the number of data points that were classified correctly and incorrectly for each class.\n",
    "\n",
    "The accuracy of a model is related to the values in its confusion matrix, but it is not the same thing. The accuracy of a model is calculated by taking the sum of the true positives and true negatives and dividing it by the total number of data points. The values in the confusion matrix represent the number of data points that were classified correctly and incorrectly for each class.\n",
    "\n",
    "For example, let's say we have a confusion matrix with the following values:\n",
    "\n",
    "Predicted\tTrue\n",
    "Positive\t10\n",
    "Negative\t2\n",
    "The accuracy of this model is 10/(10+5+2+10) = 0.67.\n",
    "\n",
    "The accuracy of the model is 0.67 because 67% of the data points were classified correctly. The values in the confusion matrix show that 10 data points were classified correctly as positive and 2 data points were classified correctly as negative.\n",
    "\n",
    "The accuracy of a model is a useful metric for evaluating the overall performance of a model. However, it is important to note that the accuracy of a model can be misleading if the dataset is imbalanced.\n",
    "\n",
    "For example, let's say we have a dataset with 90% positive data points and 10% negative data points. If we train a model on this dataset, the model will likely have a high accuracy, even if the model is not very good at classifying negative data points.\n",
    "\n",
    "This is because the model will be able to correctly classify most of the positive data points, even if it makes many mistakes when classifying negative data points.\n",
    "\n",
    "To avoid this problem, it is important to use other metrics, such as precision and recall, to evaluate the performance of a model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "743564d5-10a2-47ee-8534-8f6159af9c49",
   "metadata": {},
   "source": [
    "## Q10. How can you use a confusion matrix to identify potential biases or limitations in your machine learning model?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3c1f5ec-0c2c-49f9-837f-a8dae55b1d7a",
   "metadata": {},
   "source": [
    "A confusion matrix can be a useful tool for identifying potential biases or limitations in your machine learning model. By looking at the confusion matrix, you can see which classes the model is having difficulty with.\n",
    "\n",
    "For example, if the model has a high number of false positives for a particular class, then it may be that the model is not able to distinguish between that class and another class. This could be due to a number of factors, such as the way the data was collected or the way the model was trained.\n",
    "\n",
    "Another way to identify potential biases or limitations in your machine learning model is to look at the distribution of the data points across the different classes. If the data is not evenly distributed, then the model may be biased towards one class or another.\n",
    "\n",
    "For example, if the data set is mostly positive data points, then the model may be more likely to predict that a data point is positive, even if it is actually negative. This could lead to the model making more false positives than false negatives.\n",
    "\n",
    "It is important to note that a confusion matrix is not a perfect tool for identifying biases or limitations in your machine learning model. However, it can be a useful starting point for identifying potential problems.\n",
    "\n",
    "Here are some tips for using a confusion matrix to identify potential biases or limitations in your machine learning model:\n",
    "\n",
    "Look at the distribution of the data points across the different classes. If the data is not evenly distributed, then the model may be biased towards one class or another.\n",
    "Look at the number of false positives and false negatives. A high number of false positives or false negatives could indicate that the model is biased towards one class or another.\n",
    "Look at the values in the confusion matrix for each class. If the values in the confusion matrix are very different for one class than they are for other classes, then this could indicate that the model is biased towards that class.\n",
    "\n",
    "By following these tips, you can use a confusion matrix to identify potential biases or limitations in your machine learning model. This will help you to improve the accuracy and fairness of your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6df3929-3589-4ae1-a708-9427a2b975b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3d80837-f6c7-4b06-b759-2f8ed27711e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1099843a-6f4d-4e68-b15d-c049bbb6115b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6a71a5e-3563-4182-9d29-ed96e2fb3961",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fecb3024-e8e7-4fae-a674-9dcf62c9bf97",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b984512c-c2a5-4afd-9c57-bb8a0b07f013",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bd2ab51-be5a-4689-9759-d05ba094388b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8de1efa-1c96-4afb-a68d-7eb7e55b7dd0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27d23288-8488-4bca-bdc3-0d5bd7dcdb56",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fadd4e1-c00e-4a9b-bd59-a2a8c66b40ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec435aee-de14-46f6-ab9a-cb726a86440b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7342fd3-8696-4f30-ba92-a77a5f827fdc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ba30829-e8ce-448a-81c3-59d729392952",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a780e36b-627c-44e7-9772-d31410a3b1e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a26557a5-db1a-4af7-be79-f7ff7c585a1f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5906a336-85a8-402a-81cc-2caa1887dbbf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41c50a6e-49dc-46f9-b1c1-950bb86170ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "223f7aa6-4e5e-47b6-a8dc-8fa4f98ce6a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e3e2b3c-4008-463e-80a5-02a8c2aaab24",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "683fd554-6ff9-4627-830d-2ec9677c6353",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2858dddb-ff71-437e-b46a-fbadb0fd262f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82cded03-42ef-4426-9aa5-c6c4ccc48c6f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeeb5238-c7c1-4909-b716-5225f9203533",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40424ee8-23c3-48bb-a822-340256b6bec5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38a2d802-d32d-43d5-9856-d1adb834b929",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2955e37e-9567-455c-ab78-804f1d9c19f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a83b79f-ed08-4507-8735-1b629f30140e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9edaf45-c6ef-4206-90cf-4f5a2360daa5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e6d8167-aa96-494a-b53f-a2252af480b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de2754f3-fd1d-4bd0-8a47-a81374744790",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64dde3a9-644b-443d-8764-39a4caddd5bd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
